# Epic 1.1.2: Database Architecture Implementation

**Phase**: 1 - Foundation & Core Infrastructure  
**Epic**: 1.1.2 Database Architecture Implementation  
**Duration**: 2 weeks  
**Team Size**: 1-2 developers (Backend + Database)  

---

## Goal

Implement a robust, scalable multi-database architecture optimized for AI-augmented personal data storage, featuring PostgreSQL for structured data, vector database for embeddings, Redis for caching/queues, and proper migration/backup systems with privacy-first design.

---

## Scope Assumptions

### Database Requirements
- **Primary Database**: PostgreSQL 15+ with extensions (pgvector, uuid-ossp, pg_trgm)
- **Vector Database**: Chroma or Weaviate for embedding storage and similarity search
- **Cache/Queue**: Redis 7+ for session management, caching, and background job queues
- **Search**: PostgreSQL full-text search + vector similarity (no separate search engine initially)

### Data Characteristics
- **User Scale**: 1-10K users initially, design for 100K+ users
- **Data Volume**: 10GB-1TB per user (photos, videos, text content)
- **Query Patterns**: Heavy read (timeline browsing), moderate write (data ingestion), complex search (AI-powered)
- **Privacy**: User data isolation, encryption at rest, audit trails

### Performance Requirements
- **Read Latency**: < 100ms for timeline queries, < 500ms for AI search
- **Write Throughput**: Handle batch imports of 10K+ items
- **Availability**: 99.9% uptime, automated backups, point-in-time recovery
- **Scalability**: Horizontal scaling capability for vector database

---

## Success Criteria

- [ ] **Multi-Database Integration**: PostgreSQL, Vector DB, and Redis working together seamlessly
- [ ] **Migration System**: Automated database migrations with rollback capability
- [ ] **Performance**: Timeline queries < 100ms, vector searches < 500ms
- [ ] **Data Integrity**: ACID compliance, foreign key constraints, data validation
- [ ] **Backup & Recovery**: Automated backups, tested restore procedures
- [ ] **Monitoring**: Database performance metrics and alerting
- [ ] **Security**: Encryption at rest, connection security, user data isolation
- [ ] **Scalability**: Connection pooling, read replicas, partitioning strategy

---

## Non-Goals

- Multi-region database replication (single region for MVP)
- Advanced database sharding (vertical scaling first)
- Real-time streaming replication (batch processing acceptable)
- Complex data warehousing features (OLTP focus)
- Multi-tenant database architecture (single-tenant per user)

---

## Risks & Mitigations

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Vector DB performance at scale | High | Medium | Benchmark early, implement caching, consider alternatives |
| PostgreSQL storage growth | Medium | High | Implement data retention policies, compression, archiving |
| Complex multi-DB transactions | High | Low | Design for eventual consistency, avoid cross-DB transactions |
| Migration failures | High | Low | Comprehensive testing, rollback procedures, staging validation |
| Connection pool exhaustion | Medium | Medium | Proper connection management, monitoring, auto-scaling |

---

## Detailed Tasks

### Task 1: PostgreSQL Core Setup & Configuration

#### 1.1 PostgreSQL Installation & Extensions

**Docker Configuration (docker-compose.yml):**
```yaml
services:
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=personal_timeline
      - POSTGRES_USER=app_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/db/init:/docker-entrypoint-initdb.d
      - ./scripts/db/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U app_user -d personal_timeline"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
```

**PostgreSQL Configuration (scripts/db/postgresql.conf):**
```conf
# Connection Settings
max_connections = 200
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 4MB
maintenance_work_mem = 64MB

# Write-Ahead Logging
wal_level = replica
max_wal_size = 1GB
min_wal_size = 80MB
checkpoint_completion_target = 0.9

# Query Planner
random_page_cost = 1.1
effective_io_concurrency = 200

# Logging
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'mod'
log_min_duration_statement = 1000
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

# Performance
shared_preload_libraries = 'pg_stat_statements'
track_activity_query_size = 2048
```

**Database Initialization (scripts/db/init/01_extensions.sql):**
```sql
-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";

-- Enable vector extension (if using pgvector)
CREATE EXTENSION IF NOT EXISTS "vector";

-- Create application user with limited privileges
CREATE USER app_user WITH PASSWORD '${APP_USER_PASSWORD}';
GRANT CONNECT ON DATABASE personal_timeline TO app_user;
GRANT USAGE ON SCHEMA public TO app_user;
GRANT CREATE ON SCHEMA public TO app_user;

-- Create read-only user for analytics
CREATE USER readonly_user WITH PASSWORD '${READONLY_PASSWORD}';
GRANT CONNECT ON DATABASE personal_timeline TO readonly_user;
GRANT USAGE ON SCHEMA public TO readonly_user;
```

**Deliverables:**
- [ ] PostgreSQL 15 with required extensions
- [ ] Optimized configuration for workload
- [ ] User privilege separation
- [ ] Health checks and monitoring setup

#### 1.2 Core Database Schema

**Schema Definition (apps/api/models/database.py):**
```python
from sqlalchemy import (
    Column, String, Integer, DateTime, Text, Boolean, 
    ForeignKey, Index, UniqueConstraint, CheckConstraint,
    JSON, LargeBinary, Numeric, ARRAY
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import UUID, VECTOR
from sqlalchemy.sql import func
import uuid

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    email = Column(String(255), unique=True, nullable=False, index=True)
    password_hash = Column(String(255), nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    
    # User preferences and settings
    preferences = Column(JSON, default={})
    privacy_settings = Column(JSON, default={})
    
    # Account status
    is_active = Column(Boolean, default=True)
    is_verified = Column(Boolean, default=False)
    last_login = Column(DateTime(timezone=True))
    
    # Relationships
    memories = relationship("Memory", back_populates="user", cascade="all, delete-orphan")
    stories = relationship("Story", back_populates="user", cascade="all, delete-orphan")
    
    __table_args__ = (
        Index('idx_users_email_active', 'email', 'is_active'),
    )

class DataSource(Base):
    __tablename__ = 'data_sources'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    
    source_type = Column(String(50), nullable=False)  # 'facebook', 'instagram', etc.
    source_name = Column(String(255), nullable=False)
    import_date = Column(DateTime(timezone=True), server_default=func.now())
    
    # Import metadata
    file_path = Column(String(500))
    file_size = Column(Integer)
    total_items = Column(Integer, default=0)
    processed_items = Column(Integer, default=0)
    failed_items = Column(Integer, default=0)
    
    # Processing status
    status = Column(String(20), default='pending')  # pending, processing, completed, failed
    error_log = Column(Text)
    processing_started_at = Column(DateTime(timezone=True))
    processing_completed_at = Column(DateTime(timezone=True))
    
    # Relationships
    user = relationship("User")
    memories = relationship("Memory", back_populates="data_source")
    
    __table_args__ = (
        Index('idx_data_sources_user_type', 'user_id', 'source_type'),
        Index('idx_data_sources_status', 'status'),
    )

class Memory(Base):
    __tablename__ = 'memories'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    data_source_id = Column(UUID(as_uuid=True), ForeignKey('data_sources.id'), nullable=False)
    
    # Core memory data
    timestamp = Column(DateTime(timezone=True), nullable=False, index=True)
    content_type = Column(String(50), nullable=False)  # 'post', 'photo', 'video', 'check-in'
    title = Column(String(500))
    description = Column(Text)
    
    # Original and processed content
    original_content = Column(JSON, nullable=False)
    processed_content = Column(JSON, default={})
    
    # AI-generated data
    embedding_vector = Column(VECTOR(1536))  # OpenAI embedding dimension
    ai_summary = Column(Text)
    ai_tags = Column(ARRAY(String))
    sentiment_score = Column(Numeric(3, 2))  # -1.0 to 1.0
    
    # Location data
    latitude = Column(Numeric(10, 8))
    longitude = Column(Numeric(11, 8))
    location_name = Column(String(255))
    location_address = Column(Text)
    
    # Metadata
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    
    # Relationships
    user = relationship("User", back_populates="memories")
    data_source = relationship("DataSource", back_populates="memories")
    media_files = relationship("MediaFile", back_populates="memory", cascade="all, delete-orphan")
    memory_people = relationship("MemoryPerson", back_populates="memory", cascade="all, delete-orphan")
    
    __table_args__ = (
        Index('idx_memories_user_timestamp', 'user_id', 'timestamp'),
        Index('idx_memories_content_type', 'content_type'),
        Index('idx_memories_location', 'latitude', 'longitude'),
        Index('idx_memories_embedding', 'embedding_vector'),  # Vector similarity index
        Index('idx_memories_fulltext', 'title', 'description', postgresql_using='gin'),
    )

class MediaFile(Base):
    __tablename__ = 'media_files'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    memory_id = Column(UUID(as_uuid=True), ForeignKey('memories.id'), nullable=False)
    
    # File information
    file_type = Column(String(20), nullable=False)  # 'image', 'video', 'audio'
    original_filename = Column(String(255))
    file_path = Column(String(500), nullable=False)
    thumbnail_path = Column(String(500))
    
    # File metadata
    file_size = Column(Integer)
    mime_type = Column(String(100))
    dimensions = Column(JSON)  # {width: 1920, height: 1080}
    duration = Column(Integer)  # For videos/audio in seconds
    
    # EXIF and metadata
    exif_data = Column(JSON)
    camera_make = Column(String(100))
    camera_model = Column(String(100))
    
    # AI analysis results
    ai_analysis = Column(JSON, default={})
    detected_objects = Column(ARRAY(String))
    detected_faces = Column(JSON)  # Face detection results
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    memory = relationship("Memory", back_populates="media_files")
    
    __table_args__ = (
        Index('idx_media_files_memory', 'memory_id'),
        Index('idx_media_files_type', 'file_type'),
    )

class Person(Base):
    __tablename__ = 'people'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    
    # Person information
    name = Column(String(255), nullable=False)
    display_name = Column(String(255))
    profile_photo_url = Column(String(500))
    
    # Relationship metadata
    relationship_type = Column(String(50))  # 'family', 'friend', 'colleague', etc.
    first_appearance = Column(DateTime(timezone=True))
    last_appearance = Column(DateTime(timezone=True))
    interaction_count = Column(Integer, default=0)
    
    # AI-generated profile
    ai_generated_profile = Column(JSON, default={})
    personality_traits = Column(ARRAY(String))
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    
    # Relationships
    user = relationship("User")
    memory_people = relationship("MemoryPerson", back_populates="person")
    
    __table_args__ = (
        Index('idx_people_user_name', 'user_id', 'name'),
        UniqueConstraint('user_id', 'name', name='uq_people_user_name'),
    )

class MemoryPerson(Base):
    __tablename__ = 'memory_people'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    memory_id = Column(UUID(as_uuid=True), ForeignKey('memories.id'), nullable=False)
    person_id = Column(UUID(as_uuid=True), ForeignKey('people.id'), nullable=False)
    
    # Association metadata
    mention_type = Column(String(50))  # 'tagged', 'mentioned', 'detected'
    confidence_score = Column(Numeric(3, 2))  # AI confidence if detected
    
    # Relationships
    memory = relationship("Memory", back_populates="memory_people")
    person = relationship("Person", back_populates="memory_people")
    
    __table_args__ = (
        Index('idx_memory_people_memory', 'memory_id'),
        Index('idx_memory_people_person', 'person_id'),
        UniqueConstraint('memory_id', 'person_id', name='uq_memory_person'),
    )

class Place(Base):
    __tablename__ = 'places'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    
    # Place information
    name = Column(String(255))
    address = Column(Text)
    latitude = Column(Numeric(10, 8), nullable=False)
    longitude = Column(Numeric(11, 8), nullable=False)
    
    # Place categorization
    place_type = Column(String(50))  # 'home', 'work', 'restaurant', etc.
    google_place_id = Column(String(255))
    
    # Visit statistics
    visit_count = Column(Integer, default=0)
    first_visit = Column(DateTime(timezone=True))
    last_visit = Column(DateTime(timezone=True))
    total_time_spent = Column(Integer, default=0)  # Minutes
    
    # AI-generated insights
    significance_score = Column(Numeric(3, 2), default=0.0)
    ai_generated_description = Column(Text)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    
    # Relationships
    user = relationship("User")
    
    __table_args__ = (
        Index('idx_places_user_location', 'user_id', 'latitude', 'longitude'),
        Index('idx_places_type', 'place_type'),
    )

class Story(Base):
    __tablename__ = 'stories'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    
    # Story metadata
    title = Column(String(255), nullable=False)
    story_type = Column(String(50), nullable=False)  # 'chronological', 'thematic', etc.
    narrative_style = Column(String(50), default='memoir')
    
    # Story content
    content = Column(JSON, nullable=False)  # Generated story content
    media_timeline = Column(JSON, default={})  # Media selection and timing
    
    # Generated assets
    audio_narration_url = Column(String(500))
    video_url = Column(String(500))
    thumbnail_url = Column(String(500))
    
    # Creation metadata
    creation_parameters = Column(JSON, default={})
    generation_time_seconds = Column(Integer)
    
    # Quality and feedback
    quality_score = Column(Numeric(3, 2))
    user_rating = Column(Integer)  # 1-5 stars
    view_count = Column(Integer, default=0)
    
    # Status
    status = Column(String(20), default='draft')  # draft, published, archived
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    
    # Relationships
    user = relationship("User", back_populates="stories")
    story_memories = relationship("StoryMemory", back_populates="story", cascade="all, delete-orphan")
    
    __table_args__ = (
        Index('idx_stories_user_created', 'user_id', 'created_at'),
        Index('idx_stories_type_status', 'story_type', 'status'),
    )

class StoryMemory(Base):
    __tablename__ = 'story_memories'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    story_id = Column(UUID(as_uuid=True), ForeignKey('stories.id'), nullable=False)
    memory_id = Column(UUID(as_uuid=True), ForeignKey('memories.id'), nullable=False)
    
    # Story context
    sequence_order = Column(Integer, nullable=False)
    chapter_name = Column(String(255))
    narrative_role = Column(String(50))  # 'opening', 'climax', 'conclusion', etc.
    
    # Relationships
    story = relationship("Story", back_populates="story_memories")
    memory = relationship("Memory")
    
    __table_args__ = (
        Index('idx_story_memories_story_order', 'story_id', 'sequence_order'),
        UniqueConstraint('story_id', 'memory_id', name='uq_story_memory'),
    )

class AgentInteraction(Base):
    __tablename__ = 'agent_interactions'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('users.id'), nullable=False)
    session_id = Column(UUID(as_uuid=True), nullable=False)
    
    # Agent information
    agent_type = Column(String(50), nullable=False)  # 'archivist', 'editor', etc.
    operation = Column(String(100), nullable=False)
    
    # Input/Output
    input_data = Column(JSON, nullable=False)
    output_data = Column(JSON)
    
    # Performance metrics
    processing_time_ms = Column(Integer)
    tokens_used = Column(Integer)
    cost_usd = Column(Numeric(10, 4))
    
    # Quality metrics
    quality_score = Column(Numeric(3, 2))
    user_feedback = Column(Integer)  # 1-5 rating
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    user = relationship("User")
    
    __table_args__ = (
        Index('idx_agent_interactions_user_session', 'user_id', 'session_id'),
        Index('idx_agent_interactions_agent_type', 'agent_type'),
        Index('idx_agent_interactions_created', 'created_at'),
    )
```

**Deliverables:**
- [ ] Complete SQLAlchemy models with relationships
- [ ] Optimized indexes for query patterns
- [ ] Data validation and constraints
- [ ] Privacy-aware schema design

#### 1.3 Database Migration System

**Alembic Configuration (apps/api/alembic.ini):**
```ini
[alembic]
script_location = alembic
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = postgresql://%(DB_USER)s:%(DB_PASS)s@%(DB_HOST)s:%(DB_PORT)s/%(DB_NAME)s

[post_write_hooks]
hooks = black
black.type = console_scripts
black.entrypoint = black
black.options = -l 79 REVISION_SCRIPT_FILENAME

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**Migration Environment (apps/api/alembic/env.py):**
```python
import os
import sys
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context

# Add the project root to the path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from models.database import Base
from core.config import settings

# Alembic Config object
config = context.config

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Set the SQLAlchemy URL from environment
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    configuration = config.get_section(config.config_ini_section)
    configuration["sqlalchemy.url"] = settings.DATABASE_URL
    
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

**Migration Scripts (apps/api/scripts/db_migrate.py):**
```python
#!/usr/bin/env python3
"""
Database migration management script
"""
import os
import sys
import subprocess
import argparse
from pathlib import Path

def run_command(cmd: list, check: bool = True) -> subprocess.CompletedProcess:
    """Run a command and return the result"""
    print(f"Running: {' '.join(cmd)}")
    return subprocess.run(cmd, check=check, capture_output=True, text=True)

def create_migration(message: str) -> None:
    """Create a new migration"""
    cmd = ["alembic", "revision", "--autogenerate", "-m", message]
    result = run_command(cmd)
    
    if result.returncode == 0:
        print(f"✅ Migration created: {message}")
        print(result.stdout)
    else:
        print(f"❌ Failed to create migration: {result.stderr}")
        sys.exit(1)

def upgrade_database(revision: str = "head") -> None:
    """Upgrade database to specified revision"""
    cmd = ["alembic", "upgrade", revision]
    result = run_command(cmd)
    
    if result.returncode == 0:
        print(f"✅ Database upgraded to: {revision}")
        print(result.stdout)
    else:
        print(f"❌ Failed to upgrade database: {result.stderr}")
        sys.exit(1)

def downgrade_database(revision: str) -> None:
    """Downgrade database to specified revision"""
    cmd = ["alembic", "downgrade", revision]
    result = run_command(cmd)
    
    if result.returncode == 0:
        print(f"✅ Database downgraded to: {revision}")
        print(result.stdout)
    else:
        print(f"❌ Failed to downgrade database: {result.stderr}")
        sys.exit(1)

def show_current_revision() -> None:
    """Show current database revision"""
    cmd = ["alembic", "current"]
    result = run_command(cmd)
    
    if result.returncode == 0:
        print("Current revision:")
        print(result.stdout)
    else:
        print(f"❌ Failed to get current revision: {result.stderr}")

def show_migration_history() -> None:
    """Show migration history"""
    cmd = ["alembic", "history", "--verbose"]
    result = run_command(cmd)
    
    if result.returncode == 0:
        print("Migration history:")
        print(result.stdout)
    else:
        print(f"❌ Failed to get migration history: {result.stderr}")

def validate_migrations() -> None:
    """Validate that migrations are consistent with models"""
    # Create a temporary migration to check for differences
    cmd = ["alembic", "revision", "--autogenerate", "-m", "temp_validation", "--head", "head"]
    result = run_command(cmd, check=False)
    
    if result.returncode == 0:
        # Check if the migration file has any operations
        # If it's empty, models are in sync
        migration_files = list(Path("alembic/versions").glob("*temp_validation*.py"))
        if migration_files:
            with open(migration_files[0], 'r') as f:
                content = f.read()
                if "def upgrade():" in content and "pass" in content:
                    print("✅ Models are in sync with database")
                    os.remove(migration_files[0])
                else:
                    print("⚠️  Models have changes not reflected in migrations:")
                    print(content)
                    os.remove(migration_files[0])
                    sys.exit(1)
    else:
        print(f"❌ Failed to validate migrations: {result.stderr}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="Database migration management")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Create migration
    create_parser = subparsers.add_parser("create", help="Create a new migration")
    create_parser.add_argument("message", help="Migration message")
    
    # Upgrade
    upgrade_parser = subparsers.add_parser("upgrade", help="Upgrade database")
    upgrade_parser.add_argument("revision", nargs="?", default="head", help="Target revision")
    
    # Downgrade
    downgrade_parser = subparsers.add_parser("downgrade", help="Downgrade database")
    downgrade_parser.add_argument("revision", help="Target revision")
    
    # Current
    subparsers.add_parser("current", help="Show current revision")
    
    # History
    subparsers.add_parser("history", help="Show migration history")
    
    # Validate
    subparsers.add_parser("validate", help="Validate migrations against models")
    
    args = parser.parse_args()
    
    if args.command == "create":
        create_migration(args.message)
    elif args.command == "upgrade":
        upgrade_database(args.revision)
    elif args.command == "downgrade":
        downgrade_database(args.revision)
    elif args.command == "current":
        show_current_revision()
    elif args.command == "history":
        show_migration_history()
    elif args.command == "validate":
        validate_migrations()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
```

**Deliverables:**
- [ ] Alembic configuration for automated migrations
- [ ] Migration management scripts
- [ ] Database validation tools
- [ ] Rollback procedures and testing

### Task 2: Vector Database Integration

#### 2.1 Chroma Vector Database Setup

**Chroma Configuration (docker-compose.yml):**
```yaml
services:
  chroma:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
      - ./scripts/chroma/chroma.conf:/chroma/chroma.conf
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
      - CHROMA_DB_IMPL=clickhouse
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
    depends_on:
      - clickhouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./scripts/chroma/clickhouse-config.xml:/etc/clickhouse-server/config.xml
    environment:
      - CLICKHOUSE_DB=chroma
      - CLICKHOUSE_USER=chroma
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
    restart: unless-stopped

volumes:
  chroma_data:
  clickhouse_data:
```

**Vector Service Implementation (apps/api/services/vector_service.py):**
```python
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from uuid import UUID
import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
import openai
from tenacity import retry, stop_after_attempt, wait_exponential

from core.config import settings
from models.database import Memory

logger = logging.getLogger(__name__)

class VectorService:
    """
    Service for managing vector embeddings and similarity search
    """
    
    def __init__(self):
        self.client = None
        self.embedding_function = None
        self.collections = {}
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize Chroma client and embedding function"""
        try:
            # Initialize Chroma client
            self.client = chromadb.HttpClient(
                host=settings.CHROMA_HOST,
                port=settings.CHROMA_PORT,
                settings=Settings(
                    chroma_db_impl="clickhouse",
                    clickhouse_host=settings.CLICKHOUSE_HOST,
                    clickhouse_port=settings.CLICKHOUSE_PORT,
                )
            )
            
            # Initialize embedding function
            self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
                api_key=settings.OPENAI_API_KEY,
                model_name="text-embedding-ada-002"
            )
            
            logger.info("Vector service initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize vector service: {e}")
            raise
    
    def get_or_create_collection(self, user_id: UUID, collection_type: str = "memories"):
        """Get or create a collection for a user"""
        collection_name = f"user_{user_id}_{collection_type}"
        
        if collection_name not in self.collections:
            try:
                collection = self.client.get_or_create_collection(
                    name=collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"user_id": str(user_id), "type": collection_type}
                )
                self.collections[collection_name] = collection
                logger.info(f"Collection created/retrieved: {collection_name}")
            except Exception as e:
                logger.error(f"Failed to create collection {collection_name}: {e}")
                raise
        
        return self.collections[collection_name]
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text using OpenAI"""
        try:
            # Clean and prepare text
            cleaned_text = self._clean_text_for_embedding(text)
            
            if not cleaned_text.strip():
                logger.warning("Empty text provided for embedding")
                return [0.0] * 1536  # Return zero vector
            
            # Generate embedding using OpenAI
            response = await openai.Embedding.acreate(
                model="text-embedding-ada-002",
                input=cleaned_text
            )
            
            embedding = response['data'][0]['embedding']
            logger.debug(f"Generated embedding for text: {text[:100]}...")
            
            return embedding
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def _clean_text_for_embedding(self, text: str) -> str:
        """Clean and prepare text for embedding generation"""
        if not text:
            return ""
        
        # Remove excessive whitespace
        cleaned = " ".join(text.split())
        
        # Truncate to reasonable length (8000 chars for OpenAI)
        if len(cleaned) > 8000:
            cleaned = cleaned[:8000]
            logger.warning(f"Text truncated to 8000 characters for embedding")
        
        return cleaned
    
    async def add_memory_embedding(
        self, 
        user_id: UUID, 
        memory: Memory,
        embedding: Optional[List[float]] = None
    ) -> bool:
        """Add or update memory embedding in vector database"""
        try:
            collection = self.get_or_create_collection(user_id, "memories")
            
            # Generate embedding if not provided
            if embedding is None:
                text_content = self._extract_text_from_memory(memory)
                embedding = await self.generate_embedding(text_content)
            
            # Prepare metadata
            metadata = {
                "memory_id": str(memory.id),
                "user_id": str(user_id),
                "timestamp": memory.timestamp.isoformat(),
                "content_type": memory.content_type,
                "title": memory.title or "",
                "has_media": len(memory.media_files) > 0,
                "location": f"{memory.latitude},{memory.longitude}" if memory.latitude else None,
            }
            
            # Add to collection
            collection.add(
                embeddings=[embedding],
                documents=[text_content],
                metadatas=[metadata],
                ids=[str(memory.id)]
            )
            
            logger.info(f"Added embedding for memory {memory.id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add memory embedding: {e}")
            return False
    
    def _extract_text_from_memory(self, memory: Memory) -> str:
        """Extract searchable text content from memory"""
        text_parts = []
        
        # Add title and description
        if memory.title:
            text_parts.append(memory.title)
        if memory.description:
            text_parts.append(memory.description)
        
        # Add AI summary if available
        if memory.ai_summary:
            text_parts.append(memory.ai_summary)
        
        # Add location information
        if memory.location_name:
            text_parts.append(f"Location: {memory.location_name}")
        if memory.location_address:
            text_parts.append(f"Address: {memory.location_address}")
        
        # Add tags
        if memory.ai_tags:
            text_parts.append(f"Tags: {', '.join(memory.ai_tags)}")
        
        # Add original content if it contains text
        if memory.original_content:
            if isinstance(memory.original_content, dict):
                if 'text' in memory.original_content:
                    text_parts.append(memory.original_content['text'])
                if 'caption' in memory.original_content:
                    text_parts.append(memory.original_content['caption'])
        
        return " ".join(text_parts)
    
    async def similarity_search(
        self,
        user_id: UUID,
        query_text: str,
        limit: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Perform similarity search for memories"""
        try:
            collection = self.get_or_create_collection(user_id, "memories")
            
            # Generate query embedding
            query_embedding = await self.generate_embedding(query_text)
            
            # Build where clause for filtering
            where_clause = {"user_id": str(user_id)}
            if filters:
                if "content_types" in filters:
                    where_clause["content_type"] = {"$in": filters["content_types"]}
                if "date_range" in filters:
                    start_date, end_date = filters["date_range"]
                    where_clause["timestamp"] = {
                        "$gte": start_date.isoformat(),
                        "$lte": end_date.isoformat()
                    }
                if "has_media" in filters:
                    where_clause["has_media"] = filters["has_media"]
            
            # Perform similarity search
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=limit,
                where=where_clause,
                include=["documents", "metadatas", "distances"]
            )
            
            # Format results
            formatted_results = []
            for i in range(len(results['ids'][0])):
                result = {
                    "memory_id": results['ids'][0][i],
                    "document": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "similarity_score": 1 - results['distances'][0][i],  # Convert distance to similarity
                }
                formatted_results.append(result)
            
            logger.info(f"Found {len(formatted_results)} similar memories for query: {query_text[:100]}")
            return formatted_results
            
        except Exception as e:
            logger.error(f"Similarity search failed: {e}")
            return []
    
    async def update_memory_embedding(
        self, 
        user_id: UUID, 
        memory: Memory
    ) -> bool:
        """Update existing memory embedding"""
        try:
            collection = self.get_or_create_collection(user_id, "memories")
            
            # Generate new embedding
            text_content = self._extract_text_from_memory(memory)
            embedding = await self.generate_embedding(text_content)
            
            # Update in collection
            collection.update(
                ids=[str(memory.id)],
                embeddings=[embedding],
                documents=[text_content],
                metadatas=[{
                    "memory_id": str(memory.id),
                    "user_id": str(user_id),
                    "timestamp": memory.timestamp.isoformat(),
                    "content_type": memory.content_type,
                    "title": memory.title or "",
                    "has_media": len(memory.media_files) > 0,
                    "location": f"{memory.latitude},{memory.longitude}" if memory.latitude else None,
                }]
            )
            
            logger.info(f"Updated embedding for memory {memory.id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to update memory embedding: {e}")
            return False
    
    async def delete_memory_embedding(self, user_id: UUID, memory_id: UUID) -> bool:
        """Delete memory embedding from vector database"""
        try:
            collection = self.get_or_create_collection(user_id, "memories")
            
            collection.delete(ids=[str(memory_id)])
            
            logger.info(f"Deleted embedding for memory {memory_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete memory embedding: {e}")
            return False
    
    async def get_collection_stats(self, user_id: UUID) -> Dict[str, Any]:
        """Get statistics about user's vector collection"""
        try:
            collection = self.get_or_create_collection(user_id, "memories")
            
            count = collection.count()
            
            return {
                "total_embeddings": count,
                "collection_name": collection.name,
                "embedding_dimension": 1536,
            }
            
        except Exception as e:
            logger.error(f"Failed to get collection stats: {e}")
            return {}
    
    async def batch_add_embeddings(
        self, 
        user_id: UUID, 
        memories: List[Memory],
        batch_size: int = 100
    ) -> Tuple[int, int]:
        """Add multiple memory embeddings in batches"""
        collection = self.get_or_create_collection(user_id, "memories")
        
        successful = 0
        failed = 0
        
        for i in range(0, len(memories), batch_size):
            batch = memories[i:i + batch_size]
            
            try:
                # Prepare batch data
                embeddings = []
                documents = []
                metadatas = []
                ids = []
                
                for memory in batch:
                    text_content = self._extract_text_from_memory(memory)
                    embedding = await self.generate_embedding(text_content)
                    
                    embeddings.append(embedding)
                    documents.append(text_content)
                    metadatas.append({
                        "memory_id": str(memory.id),
                        "user_id": str(user_id),
                        "timestamp": memory.timestamp.isoformat(),
                        "content_type": memory.content_type,
                        "title": memory.title or "",
                        "has_media": len(memory.media_files) > 0,
                        "location": f"{memory.latitude},{memory.longitude}" if memory.latitude else None,
                    })
                    ids.append(str(memory.id))
                
                # Add batch to collection
                collection.add(
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                
                successful += len(batch)
                logger.info(f"Added batch of {len(batch)} embeddings")
                
                # Small delay to avoid overwhelming the service
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Failed to add batch: {e}")
                failed += len(batch)
        
        logger.info(f"Batch embedding complete: {successful} successful, {failed} failed")
        return successful, failed

# Global vector service instance
vector_service = VectorService()
```

**Deliverables:**
- [ ] Chroma vector database with ClickHouse backend
- [ ] Vector service with OpenAI embedding integration
- [ ] Batch processing for large datasets
- [ ] Similarity search with filtering capabilities

### Task 3: Redis Cache & Queue Setup

#### 3.1 Redis Configuration & Services

**Redis Configuration (scripts/redis/redis.conf):**
```conf
# Network
bind 0.0.0.0
port 6379
protected-mode no

# General
daemonize no
supervised no
pidfile /var/run/redis_6379.pid
loglevel notice
logfile ""

# Snapshotting
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir ./

# Replication
replica-serve-stale-data yes
replica-read-only yes

# Security
requirepass ${REDIS_PASSWORD}

# Memory management
maxmemory 256mb
maxmemory-policy allkeys-lru

# Append only file
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Slow log
slowlog-log-slower-than 10000
slowlog-max-len 128

# Client output buffer limits
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
```

**Redis Service Implementation (apps/api/services/redis_service.py):**
```python
import asyncio
import json
import logging
from typing import Any, Dict, List, Optional, Union
from datetime import datetime, timedelta
import redis.asyncio as redis
from redis.asyncio import ConnectionPool
import pickle

from core.config import settings

logger = logging.getLogger(__name__)

class RedisService:
    """
    Redis service for caching, sessions, and job queues
    """
    
    def __init__(self):
        self.pool = None
        self.client = None
        self._initialize_connection()
    
    def _initialize_connection(self):
        """Initialize Redis connection pool"""
        try:
            self.pool = ConnectionPool.from_url(
                settings.REDIS_URL,
                max_connections=20,
                retry_on_timeout=True,
                socket_keepalive=True,
                socket_keepalive_options={},
                health_check_interval=30,
            )
            
            self.client = redis.Redis(connection_pool=self.pool)
            logger.info("Redis service initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Redis service: {e}")
            raise
    
    async def ping(self) -> bool:
        """Test Redis connection"""
        try:
            await self.client.ping()
            return True
        except Exception as e:
            logger.error(f"Redis ping failed: {e}")
            return False
    
    # Cache Operations
    async def set_cache(
        self, 
        key: str, 
        value: Any, 
        expire: Optional[int] = None,
        serialize: bool = True
    ) -> bool:
        """Set cache value with optional expiration"""
        try:
            if serialize:
                if isinstance(value, (dict, list)):
                    value = json.dumps(value, default=str)
                elif not isinstance(value, (str, bytes, int, float)):
                    value = pickle.dumps(value)
            
            result = await self.client.set(key, value, ex=expire)
            logger.debug(f"Cache set: {key} (expire: {expire})")
            return result
            
        except Exception as e:
            logger.error(f"Failed to set cache {key}: {e}")
            return False
    
    async def get_cache(
        self, 
        key: str, 
        deserialize: bool = True,
        default: Any = None
    ) -> Any:
        """Get cache value with optional deserialization"""
        try:
            value = await self.client.get(key)
            
            if value is None:
                return default
            
            if deserialize and isinstance(value, bytes):
                try:
                    # Try JSON first
                    return json.loads(value.decode('utf-8'))
                except (json.JSONDecodeError, UnicodeDecodeError):
                    try:
                        # Try pickle
                        return pickle.loads(value)
                    except:
                        # Return as string
                        return value.decode('utf-8')
            
            return value
            
        except Exception as e:
            logger.error(f"Failed to get cache {key}: {e}")
            return default
    
    async def delete_cache(self, key: str) -> bool:
        """Delete cache key"""
        try:
            result = await self.client.delete(key)
            logger.debug(f"Cache deleted: {key}")
            return bool(result)
            
        except Exception as e:
            logger.error(f"Failed to delete cache {key}: {e}")
            return False
    
    async def exists_cache(self, key: str) -> bool:
        """Check if cache key exists"""
        try:
            result = await self.client.exists(key)
            return bool(result)
            
        except Exception as e:
            logger.error(f"Failed to check cache existence {key}: {e}")
            return False
    
    async def set_cache_many(
        self, 
        mapping: Dict[str, Any], 
        expire: Optional[int] = None
    ) -> bool:
        """Set multiple cache values"""
        try:
            pipe = self.client.pipeline()
            
            for key, value in mapping.items():
                if isinstance(value, (dict, list)):
                    value = json.dumps(value, default=str)
                elif not isinstance(value, (str, bytes, int, float)):
                    value = pickle.dumps(value)
                
                pipe.set(key, value, ex=expire)
            
            await pipe.execute()
            logger.debug(f"Cache set many: {len(mapping)} keys")
            return True
            
        except Exception as e:
            logger.error(f"Failed to set cache many: {e}")
            return False
    
    async def get_cache_many(self, keys: List[str]) -> Dict[str, Any]:
        """Get multiple cache values"""
        try:
            values = await self.client.mget(keys)
            result = {}
            
            for key, value in zip(keys, values):
                if value is not None:
                    try:
                        if isinstance(value, bytes):
                            try:
                                result[key] = json.loads(value.decode('utf-8'))
                            except (json.JSONDecodeError, UnicodeDecodeError):
                                try:
                                    result[key] = pickle.loads(value)
                                except:
                                    result[key] = value.decode('utf-8')
                        else:
                            result[key] = value
                    except Exception:
                        result[key] = value
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to get cache many: {e}")
            return {}
    
    # Session Management
    async def create_session(
        self, 
        session_id: str, 
        user_id: str, 
        data: Dict[str, Any],
        expire: int = 3600
    ) -> bool:
        """Create user session"""
        session_key = f"session:{session_id}"
        session_data = {
            "user_id": user_id,
            "created_at": datetime.utcnow().isoformat(),
            "data": data
        }
        
        return await self.set_cache(session_key, session_data, expire=expire)
    
    async def get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Get session data"""
        session_key = f"session:{session_id}"
        return await self.get_cache(session_key)
    
    async def update_session(
        self, 
        session_id: str, 
        data: Dict[str, Any],
        extend_expire: int = 3600
    ) -> bool:
        """Update session data and extend expiration"""
        session_key = f"session:{session_id}"
        session_data = await self.get_session(session_id)
        
        if session_data:
            session_data["data"].update(data)
            session_data["updated_at"] = datetime.utcnow().isoformat()
            return await self.set_cache(session_key, session_data, expire=extend_expire)
        
        return False
    
    async def delete_session(self, session_id: str) -> bool:
        """Delete session"""
        session_key = f"session:{session_id}"
        return await self.delete_cache(session_key)
    
    # Job Queue Operations
    async def enqueue_job(
        self, 
        queue_name: str, 
        job_data: Dict[str, Any],
        priority: int = 0
    ) -> bool:
        """Add job to queue with priority"""
        try:
            job_payload = {
                "id": f"job_{datetime.utcnow().timestamp()}",
                "data": job_data,
                "created_at": datetime.utcnow().isoformat(),
                "priority": priority
            }
            
            # Use sorted set for priority queue
            score = priority * -1  # Higher priority = lower score
            result = await self.client.zadd(
                f"queue:{queue_name}", 
                {json.dumps(job_payload, default=str): score}
            )
            
            logger.debug(f"Job enqueued to {queue_name}: {job_payload['id']}")
            return bool(result)
            
        except Exception as e:
            logger.error(f"Failed to enqueue job: {e}")
            return False
    
    async def dequeue_job(self, queue_name: str) -> Optional[Dict[str, Any]]:
        """Get next job from queue (highest priority first)"""
        try:
            # Get highest priority job (lowest score)
            result = await self.client.zpopmin(f"queue:{queue_name}")
            
            if result:
                job_data, score = result[0]
                job_payload = json.loads(job_data)
                logger.debug(f"Job dequeued from {queue_name}: {job_payload['id']}")
                return job_payload
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to dequeue job: {e}")
            return None
    
    async def get_queue_size(self, queue_name: str) -> int:
        """Get number of jobs in queue"""
        try:
            size = await self.client.zcard(f"queue:{queue_name}")
            return size
            
        except Exception as e:
            logger.error(f"Failed to get queue size: {e}")
            return 0
    
    async def clear_queue(self, queue_name: str) -> bool:
        """Clear all jobs from queue"""
        try:
            result = await self.client.delete(f"queue:{queue_name}")
            logger.info(f"Queue cleared: {queue_name}")
            return bool(result)
            
        except Exception as e:
            logger.error(f"Failed to clear queue: {e}")
            return False
    
    # Rate Limiting
    async def check_rate_limit(
        self, 
        key: str, 
        limit: int, 
        window: int
    ) -> Tuple[bool, int, int]:
        """Check rate limit using sliding window"""
        try:
            now = datetime.utcnow().timestamp()
            window_start = now - window
            
            pipe = self.client.pipeline()
            
            # Remove old entries
            pipe.zremrangebyscore(f"rate_limit:{key}", 0, window_start)
            
            # Count current requests
            pipe.zcard(f"rate_limit:{key}")
            
            # Add current request
            pipe.zadd(f"rate_limit:{key}", {str(now): now})
            
            # Set expiration
            pipe.expire(f"rate_limit:{key}", window + 1)
            
            results = await pipe.execute()
            current_count = results[1]
            
            allowed = current_count < limit
            remaining = max(0, limit - current_count - 1)
            
            return allowed, remaining, window
            
        except Exception as e:
            logger.error(f"Rate limit check failed: {e}")
            return True, limit, window  # Allow on error
    
    # Pub/Sub Operations
    async def publish(self, channel: str, message: Any) -> int:
        """Publish message to channel"""
        try:
            if not isinstance(message, (str, bytes)):
                message = json.dumps(message, default=str)
            
            result = await self.client.publish(channel, message)
            logger.debug(f"Message published to {channel}")
            return result
            
        except Exception as e:
            logger.error(f"Failed to publish message: {e}")
            return 0
    
    async def subscribe(self, channels: List[str]) -> redis.client.PubSub:
        """Subscribe to channels"""
        try:
            pubsub = self.client.pubsub()
            await pubsub.subscribe(*channels)
            logger.info(f"Subscribed to channels: {channels}")
            return pubsub
            
        except Exception as e:
            logger.error(f"Failed to subscribe: {e}")
            raise
    
    # Health and Statistics
    async def get_info(self) -> Dict[str, Any]:
        """Get Redis server information"""
        try:
            info = await self.client.info()
            return info
            
        except Exception as e:
            logger.error(f"Failed to get Redis info: {e}")
            return {}
    
    async def get_memory_usage(self) -> Dict[str, Any]:
        """Get Redis memory usage statistics"""
        try:
            info = await self.client.info("memory")
            return {
                "used_memory": info.get("used_memory", 0),
                "used_memory_human": info.get("used_memory_human", "0B"),
                "used_memory_peak": info.get("used_memory_peak", 0),
                "used_memory_peak_human": info.get("used_memory_peak_human", "0B"),
                "maxmemory": info.get("maxmemory", 0),
                "maxmemory_human": info.get("maxmemory_human", "0B"),
            }
            
        except Exception as e:
            logger.error(f"Failed to get memory usage: {e}")
            return {}
    
    async def close(self):
        """Close Redis connection"""
        if self.client:
            await self.client.close()
        if self.pool:
            await self.pool.disconnect()

# Global Redis service instance
redis_service = RedisService()
```

**Deliverables:**
- [ ] Redis configuration optimized for caching and queues
- [ ] Comprehensive Redis service with caching, sessions, and job queues
- [ ] Rate limiting and pub/sub capabilities
- [ ] Connection pooling and health monitoring

### Task 4: Database Performance Optimization

#### 4.1 Connection Pooling & Query Optimization

**Database Connection Management (apps/api/core/database.py):**
```python
import logging
from typing import AsyncGenerator
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
from sqlalchemy.pool import QueuePool
from sqlalchemy import event, text
from sqlalchemy.engine import Engine
import time

from core.config import settings

logger = logging.getLogger(__name__)

# Create async engine with connection pooling
engine = create_async_engine(
    settings.DATABASE_URL,
    
    # Connection pool settings
    poolclass=QueuePool,
    pool_size=20,  # Number of connections to maintain
    max_overflow=30,  # Additional connections beyond pool_size
    pool_pre_ping=True,  # Validate connections before use
    pool_recycle=3600,  # Recycle connections after 1 hour
    
    # Query optimization
    echo=settings.DEBUG,  # Log SQL queries in debug mode
    echo_pool=settings.DEBUG,  # Log connection pool events
    
    # Performance settings
    connect_args={
        "server_settings": {
            "application_name": "personal_timeline_api",
            "jit": "off",  # Disable JIT for consistent performance
        }
    }
)

# Create session factory
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=True,
    autocommit=False,
)

# Query performance monitoring
@event.listens_for(Engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    context._query_start_time = time.time()

@event.listens_for(Engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - context._query_start_time
    
    # Log slow queries
    if total > 0.5:  # Log queries taking more than 500ms
        logger.warning(
            f"Slow query detected: {total:.3f}s - {statement[:200]}...",
            extra={
                "query_time": total,
                "query": statement[:500],
                "parameters": str(parameters)[:200] if parameters else None
            }
        )

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """Dependency to get database session"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

async def init_db():
    """Initialize database and create tables"""
    from models.database import Base
    
    async with engine.begin() as conn:
        # Create all tables
        await conn.run_sync(Base.metadata.create_all)
        
        # Create indexes
        await conn.execute(text("""
            -- Create partial indexes for better performance
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_memories_user_recent 
            ON memories (user_id, timestamp DESC) 
            WHERE timestamp > NOW() - INTERVAL '1 year';
            
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_memories_user_photos 
            ON memories (user_id, timestamp DESC) 
            WHERE content_type = 'photo';
            
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_memories_location_recent 
            ON memories (latitude, longitude, timestamp DESC) 
            WHERE latitude IS NOT NULL AND longitude IS NOT NULL 
            AND timestamp > NOW() - INTERVAL '1 year';
            
            -- Create GIN index for full-text search
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_memories_search 
            ON memories USING gin(to_tsvector('english', 
                COALESCE(title, '') || ' ' || COALESCE(description, '')));
        """))
        
        logger.info("Database initialized successfully")

class DatabaseHealthCheck:
    """Database health monitoring"""
    
    @staticmethod
    async def check_connection() -> bool:
        """Check if database connection is healthy"""
        try:
            async with AsyncSessionLocal() as session:
                result = await session.execute(text("SELECT 1"))
                return result.scalar() == 1
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return False
    
    @staticmethod
    async def get_connection_stats() -> dict:
        """Get connection pool statistics"""
        try:
            pool = engine.pool
            return {
                "pool_size": pool.size(),
                "checked_in": pool.checkedin(),
                "checked_out": pool.checkedout(),
                "overflow": pool.overflow(),
                "invalid": pool.invalid(),
            }
        except Exception as e:
            logger.error(f"Failed to get connection stats: {e}")
            return {}
    
    @staticmethod
    async def get_slow_queries(limit: int = 10) -> list:
        """Get slow queries from pg_stat_statements"""
        try:
            async with AsyncSessionLocal() as session:
                result = await session.execute(text("""
                    SELECT 
                        query,
                        calls,
                        total_exec_time,
                        mean_exec_time,
                        max_exec_time,
                        rows
                    FROM pg_stat_statements 
                    WHERE query NOT LIKE '%pg_stat_statements%'
                    ORDER BY mean_exec_time DESC 
                    LIMIT :limit
                """), {"limit": limit})
                
                return [dict(row) for row in result]
        except Exception as e:
            logger.error(f"Failed to get slow queries: {e}")
            return []
```

**Query Optimization Utilities (apps/api/utils/query_utils.py):**
```python
from typing import List, Dict, Any, Optional, Tuple
from sqlalchemy import text, func, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload, joinedload
from datetime import datetime, timedelta

from models.database import Memory, MediaFile, Person, MemoryPerson

class QueryOptimizer:
    """Utilities for optimized database queries"""
    
    @staticmethod
    async def get_timeline_memories(
        session: AsyncSession,
        user_id: str,
        limit: int = 50,
        offset: int = 0,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        content_types: Optional[List[str]] = None
    ) -> Tuple[List[Memory], int]:
        """Optimized timeline query with pagination"""
        
        # Build base query with eager loading
        query = session.query(Memory).options(
            selectinload(Memory.media_files),
            selectinload(Memory.memory_people).selectinload(MemoryPerson.person)
        ).filter(Memory.user_id == user_id)
        
        # Add filters
        if start_date:
            query = query.filter(Memory.timestamp >= start_date)
        if end_date:
            query = query.filter(Memory.timestamp <= end_date)
        if content_types:
            query = query.filter(Memory.content_type.in_(content_types))
        
        # Order by timestamp descending
        query = query.order_by(Memory.timestamp.desc())
        
        # Get total count for pagination
        count_query = query.statement.with_only_columns(func.count()).order_by(None)
        total_count = await session.execute(count_query)
        total = total_count.scalar()
        
        # Apply pagination
        query = query.offset(offset).limit(limit)
        
        # Execute query
        result = await session.execute(query)
        memories = result.scalars().all()
        
        return memories, total
    
    @staticmethod
    async def search_memories_fulltext(
        session: AsyncSession,
        user_id: str,
        search_query: str,
        limit: int = 20
    ) -> List[Memory]:
        """Full-text search using PostgreSQL's built-in search"""
        
        # Use PostgreSQL's full-text search
        query = session.query(Memory).filter(
            and_(
                Memory.user_id == user_id,
                func.to_tsvector('english', 
                    func.coalesce(Memory.title, '') + ' ' + 
                    func.coalesce(Memory.description, '')
                ).match(search_query)
            )
        ).order_by(
            func.ts_rank(
                func.to_tsvector('english', 
                    func.coalesce(Memory.title, '') + ' ' + 
                    func.coalesce(Memory.description, '')
                ),
                func.plainto_tsquery('english', search_query)
            ).desc()
        ).limit(limit)
        
        result = await session.execute(query)
        return result.scalars().all()
    
    @staticmethod
    async def get_memories_by_location(
        session: AsyncSession,
        user_id: str,
        latitude: float,
        longitude: float,
        radius_km: float = 1.0,
        limit: int = 20
    ) -> List[Memory]:
        """Get memories within a geographic radius"""
        
        # Use PostgreSQL's earth distance function
        query = session.query(Memory).filter(
            and_(
                Memory.user_id == user_id,
                Memory.latitude.isnot(None),
                Memory.longitude.isnot(None),
                text("""
                    earth_distance(
                        ll_to_earth(:lat, :lng),
                        ll_to_earth(latitude, longitude)
                    ) <= :radius
                """)
            )
        ).params(
            lat=latitude,
            lng=longitude,
            radius=radius_km * 1000  # Convert km to meters
        ).order_by(Memory.timestamp.desc()).limit(limit)
        
        result = await session.execute(query)
        return result.scalars().all()
    
    @staticmethod
    async def get_memory_statistics(
        session: AsyncSession,
        user_id: str
    ) -> Dict[str, Any]:
        """Get user's memory statistics"""
        
        # Single query to get multiple statistics
        stats_query = text("""
            SELECT 
                COUNT(*) as total_memories,
                COUNT(CASE WHEN content_type = 'photo' THEN 1 END) as photo_count,
                COUNT(CASE WHEN content_type = 'video' THEN 1 END) as video_count,
                COUNT(CASE WHEN content_type = 'post' THEN 1 END) as post_count,
                COUNT(CASE WHEN latitude IS NOT NULL THEN 1 END) as geotagged_count,
                MIN(timestamp) as earliest_memory,
                MAX(timestamp) as latest_memory,
                COUNT(DISTINCT DATE(timestamp)) as active_days
            FROM memories 
            WHERE user_id = :user_id
        """)
        
        result = await session.execute(stats_query, {"user_id": user_id})
        row = result.fetchone()
        
        return {
            "total_memories": row.total_memories,
            "photo_count": row.photo_count,
            "video_count": row.video_count,
            "post_count": row.post_count,
            "geotagged_count": row.geotagged_count,
            "earliest_memory": row.earliest_memory,
            "latest_memory": row.latest_memory,
            "active_days": row.active_days,
        }
    
    @staticmethod
    async def get_activity_heatmap(
        session: AsyncSession,
        user_id: str,
        year: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """Get activity heatmap data for calendar visualization"""
        
        year_filter = ""
        params = {"user_id": user_id}
        
        if year:
            year_filter = "AND EXTRACT(year FROM timestamp) = :year"
            params["year"] = year
        
        heatmap_query = text(f"""
            SELECT 
                DATE(timestamp) as date,
                COUNT(*) as activity_count,
                COUNT(CASE WHEN content_type = 'photo' THEN 1 END) as photo_count,
                COUNT(CASE WHEN content_type = 'post' THEN 1 END) as post_count
            FROM memories 
            WHERE user_id = :user_id {year_filter}
            GROUP BY DATE(timestamp)
            ORDER BY date
        """)
        
        result = await session.execute(heatmap_query, params)
        
        return [
            {
                "date": row.date.isoformat(),
                "activity_count": row.activity_count,
                "photo_count": row.photo_count,
                "post_count": row.post_count,
            }
            for row in result
        ]
    
    @staticmethod
    async def get_top_people(
        session: AsyncSession,
        user_id: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """Get most frequently mentioned people"""
        
        people_query = text("""
            SELECT 
                p.id,
                p.name,
                p.profile_photo_url,
                COUNT(mp.memory_id) as mention_count,
                MIN(m.timestamp) as first_mention,
                MAX(m.timestamp) as last_mention
            FROM people p
            JOIN memory_people mp ON p.id = mp.person_id
            JOIN memories m ON mp.memory_id = m.id
            WHERE p.user_id = :user_id
            GROUP BY p.id, p.name, p.profile_photo_url
            ORDER BY mention_count DESC
            LIMIT :limit
        """)
        
        result = await session.execute(people_query, {"user_id": user_id, "limit": limit})
        
        return [
            {
                "id": str(row.id),
                "name": row.name,
                "profile_photo_url": row.profile_photo_url,
                "mention_count": row.mention_count,
                "first_mention": row.first_mention.isoformat() if row.first_mention else None,
                "last_mention": row.last_mention.isoformat() if row.last_mention else None,
            }
            for row in result
        ]
```

**Deliverables:**
- [ ] Optimized connection pooling configuration
- [ ] Query performance monitoring and logging
- [ ] Specialized indexes for common query patterns
- [ ] Query optimization utilities and helpers
- [ ] Database health monitoring and statistics

### Task 5: Backup & Recovery System

#### 5.1 Automated Backup Strategy

**Backup Scripts (scripts/db/backup.sh):**
```bash
#!/bin/bash
set -e

# Configuration
BACKUP_DIR="/backups"
RETENTION_DAYS=30
POSTGRES_HOST="${POSTGRES_HOST:-localhost}"
POSTGRES_PORT="${POSTGRES_PORT:-5432}"
POSTGRES_DB="${POSTGRES_DB:-personal_timeline}"
POSTGRES_USER="${POSTGRES_USER:-app_user}"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

# Create backup directory
mkdir -p "$BACKUP_DIR"

echo "🗄️  Starting database backup..."

# Full database backup
echo "Creating full database backup..."
pg_dump \
    --host="$POSTGRES_HOST" \
    --port="$POSTGRES_PORT" \
    --username="$POSTGRES_USER" \
    --dbname="$POSTGRES_DB" \
    --format=custom \
    --compress=9 \
    --verbose \
    --file="$BACKUP_DIR/full_backup_$TIMESTAMP.dump"

# Schema-only backup
echo "Creating schema-only backup..."
pg_dump \
    --host="$POSTGRES_HOST" \
    --port="$POSTGRES_PORT" \
    --username="$POSTGRES_USER" \
    --dbname="$POSTGRES_DB" \
    --schema-only \
    --format=plain \
    --file="$BACKUP_DIR/schema_backup_$TIMESTAMP.sql"

# Data-only backup for critical tables
echo "Creating data-only backup for critical tables..."
pg_dump \
    --host="$POSTGRES_HOST" \
    --port="$POSTGRES_PORT" \
    --username="$POSTGRES_USER" \
    --dbname="$POSTGRES_DB" \
    --data-only \
    --format=custom \
    --compress=9 \
    --table=users \
    --table=memories \
    --table=stories \
    --file="$BACKUP_DIR/critical_data_$TIMESTAMP.dump"

# Backup Redis data
echo "Creating Redis backup..."
redis-cli --rdb "$BACKUP_DIR/redis_backup_$TIMESTAMP.rdb"

# Backup Chroma data
echo "Creating Chroma backup..."
tar -czf "$BACKUP_DIR/chroma_backup_$TIMESTAMP.tar.gz" -C /chroma/chroma .

# Create backup manifest
cat > "$BACKUP_DIR/backup_manifest_$TIMESTAMP.json" << EOF
{
    "timestamp": "$TIMESTAMP",
    "date": "$(date -Iseconds)",
    "files": {
        "full_database": "full_backup_$TIMESTAMP.dump",
        "schema_only": "schema_backup_$TIMESTAMP.sql",
        "critical_data": "critical_data_$TIMESTAMP.dump",
        "redis": "redis_backup_$TIMESTAMP.rdb",
        "chroma": "chroma_backup_$TIMESTAMP.tar.gz"
    },
    "sizes": {
        "full_database": "$(du -h $BACKUP_DIR/full_backup_$TIMESTAMP.dump | cut -f1)",
        "schema_only": "$(du -h $BACKUP_DIR/schema_backup_$TIMESTAMP.sql | cut -f1)",
        "critical_data": "$(du -h $BACKUP_DIR/critical_data_$TIMESTAMP.dump | cut -f1)",
        "redis": "$(du -h $BACKUP_DIR/redis_backup_$TIMESTAMP.rdb | cut -f1)",
        "chroma": "$(du -h $BACKUP_DIR/chroma_backup_$TIMESTAMP.tar.gz | cut -f1)"
    }
}
EOF

# Cleanup old backups
echo "Cleaning up old backups (older than $RETENTION_DAYS days)..."
find "$BACKUP_DIR" -name "*.dump" -mtime +$RETENTION_DAYS -delete
find "$BACKUP_DIR" -name "*.sql" -mtime +$RETENTION_DAYS -delete
find "$BACKUP_DIR" -name "*.rdb" -mtime +$RETENTION_DAYS -delete
find "$BACKUP_DIR" -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete
find "$BACKUP_DIR" -name "*.json" -mtime +$RETENTION_DAYS -delete

# Upload to S3 (if configured)
if [ -n "$AWS_S3_BACKUP_BUCKET" ]; then
    echo "Uploading backups to S3..."
    aws s3 sync "$BACKUP_DIR" "s3://$AWS_S3_BACKUP_BUCKET/backups/$(date +%Y/%m/%d)/" \
        --exclude "*" \
        --include "*$TIMESTAMP*" \
        --storage-class STANDARD_IA
fi

echo "✅ Backup completed successfully!"
echo "Backup files:"
ls -lh "$BACKUP_DIR"/*$TIMESTAMP*
```

**Restore Scripts (scripts/db/restore.sh):**
```bash
#!/bin/bash
set -e

# Configuration
BACKUP_DIR="/backups"
POSTGRES_HOST="${POSTGRES_HOST:-localhost}"
POSTGRES_PORT="${POSTGRES_PORT:-5432}"
POSTGRES_DB="${POSTGRES_DB:-personal_timeline}"
POSTGRES_USER="${POSTGRES_USER:-app_user}"

# Function to list available backups
list_backups() {
    echo "Available backups:"
    ls -la "$BACKUP_DIR"/backup_manifest_*.json | while read -r manifest; do
        timestamp=$(basename "$manifest" | sed 's/backup_manifest_\(.*\)\.json/\1/')
        date=$(jq -r '.date' "$manifest")
        echo "  $timestamp - $date"
    done
}

# Function to restore from backup
restore_backup() {
    local timestamp="$1"
    local restore_type="${2:-full}"
    
    if [ -z "$timestamp" ]; then
        echo "❌ Please specify a backup timestamp"
        list_backups
        exit 1
    fi
    
    local manifest="$BACKUP_DIR/backup_manifest_$timestamp.json"
    if [ ! -f "$manifest" ]; then
        echo "❌ Backup manifest not found: $manifest"
        list_backups
        exit 1
    fi
    
    echo "🔄 Starting restore from backup: $timestamp"
    echo "Restore type: $restore_type"
    
    # Confirm restore
    read -p "⚠️  This will overwrite the current database. Are you sure? (y/N) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo "❌ Restore cancelled"
        exit 1
    fi
    
    case "$restore_type" in
        "full")
            restore_full_database "$timestamp"
            ;;
        "schema")
            restore_schema_only "$timestamp"
            ;;
        "data")
            restore_data_only "$timestamp"
            ;;
        "redis")
            restore_redis "$timestamp"
            ;;
        "chroma")
            restore_chroma "$timestamp"
            ;;
        *)
            echo "❌ Invalid restore type: $restore_type"
            echo "Valid types: full, schema, data, redis, chroma"
            exit 1
            ;;
    esac
}

restore_full_database() {
    local timestamp="$1"
    local backup_file="$BACKUP_DIR/full_backup_$timestamp.dump"
    
    if [ ! -f "$backup_file" ]; then
        echo "❌ Backup file not found: $backup_file"
        exit 1
    fi
    
    echo "Dropping existing database..."
    dropdb --host="$POSTGRES_HOST" --port="$POSTGRES_PORT" --username="$POSTGRES_USER" "$POSTGRES_DB" || true
    
    echo "Creating new database..."
    createdb --host="$POSTGRES_HOST" --port="$POSTGRES_PORT" --username="$POSTGRES_USER" "$POSTGRES_DB"
    
    echo "Restoring database from backup..."
    pg_restore \
        --host="$POSTGRES_HOST" \
        --port="$POSTGRES_PORT" \
        --username="$POSTGRES_USER" \
        --dbname="$POSTGRES_DB" \
        --verbose \
        --clean \
        --if-exists \
        "$backup_file"
    
    echo "✅ Full database restore completed"
}

restore_schema_only() {
    local timestamp="$1"
    local backup_file="$BACKUP_DIR/schema_backup_$timestamp.sql"
    
    if [ ! -f "$backup_file" ]; then
        echo "❌ Schema backup file not found: $backup_file"
        exit 1
    fi
    
    echo "Restoring schema from backup..."
    psql \
        --host="$POSTGRES_HOST" \
        --port="$POSTGRES_PORT" \
        --username="$POSTGRES_USER" \
        --dbname="$POSTGRES_DB" \
        --file="$backup_file"
    
    echo "✅ Schema restore completed"
}

restore_data_only() {
    local timestamp="$1"
    local backup_file="$BACKUP_DIR/critical_data_$timestamp.dump"
    
    if [ ! -f "$backup_file" ]; then
        echo "❌ Data backup file not found: $backup_file"
        exit 1
    fi
    
    echo "Restoring critical data from backup..."
    pg_restore \
        --host="$POSTGRES_HOST" \
        --port="$POSTGRES_PORT" \
        --username="$POSTGRES_USER" \
        --dbname="$POSTGRES_DB" \
        --verbose \
        --data-only \
        "$backup_file"
    
    echo "✅ Data restore completed"
}

restore_redis() {
    local timestamp="$1"
    local backup_file="$BACKUP_DIR/redis_backup_$timestamp.rdb"
    
    if [ ! -f "$backup_file" ]; then
        echo "❌ Redis backup file not found: $backup_file"
        exit 1
    fi
    
    echo "Stopping Redis..."
    redis-cli SHUTDOWN NOSAVE || true
    
    echo "Restoring Redis data..."
    cp "$backup_file" /var/lib/redis/dump.rdb
    chown redis:redis /var/lib/redis/dump.rdb
    
    echo "Starting Redis..."
    systemctl start redis
    
    echo "✅ Redis restore completed"
}

restore_chroma() {
    local timestamp="$1"
    local backup_file="$BACKUP_DIR/chroma_backup_$timestamp.tar.gz"
    
    if [ ! -f "$backup_file" ]; then
        echo "❌ Chroma backup file not found: $backup_file"
        exit 1
    fi
    
    echo "Stopping Chroma..."
    docker-compose stop chroma
    
    echo "Restoring Chroma data..."
    rm -rf /chroma/chroma/*
    tar -xzf "$backup_file" -C /chroma/chroma/
    
    echo "Starting Chroma..."
    docker-compose start chroma
    
    echo "✅ Chroma restore completed"
}

# Function to test backup integrity
test_backup() {
    local timestamp="$1"
    
    if [ -z "$timestamp" ]; then
        echo "❌ Please specify a backup timestamp"
        list_backups
        exit 1
    fi
    
    local manifest="$BACKUP_DIR/backup_manifest_$timestamp.json"
    if [ ! -f "$manifest" ]; then
        echo "❌ Backup manifest not found: $manifest"
        exit 1
    fi
    
    echo "🧪 Testing backup integrity: $timestamp"
    
    # Test PostgreSQL backup
    local pg_backup="$BACKUP_DIR/full_backup_$timestamp.dump"
    if [ -f "$pg_backup" ]; then
        echo "Testing PostgreSQL backup..."
        pg_restore --list "$pg_backup" > /dev/null
        echo "✅ PostgreSQL backup is valid"
    fi
    
    # Test Redis backup
    local redis_backup="$BACKUP_DIR/redis_backup_$timestamp.rdb"
    if [ -f "$redis_backup" ]; then
        echo "Testing Redis backup..."
        redis-check-rdb "$redis_backup"
        echo "✅ Redis backup is valid"
    fi
    
    # Test Chroma backup
    local chroma_backup="$BACKUP_DIR/chroma_backup_$timestamp.tar.gz"
    if [ -f "$chroma_backup" ]; then
        echo "Testing Chroma backup..."
        tar -tzf "$chroma_backup" > /dev/null
        echo "✅ Chroma backup is valid"
    fi
    
    echo "✅ All backup files are valid"
}

# Main script logic
case "${1:-}" in
    "list")
        list_backups
        ;;
    "restore")
        restore_backup "$2" "$3"
        ;;
    "test")
        test_backup "$2"
        ;;
    *)
        echo "Usage: $0 {list|restore|test}"
        echo ""
        echo "Commands:"
        echo "  list                     - List available backups"
        echo "  restore <timestamp> [type] - Restore from backup"
        echo "  test <timestamp>         - Test backup integrity"
        echo ""
        echo "Restore types: full, schema, data, redis, chroma"
        exit 1
        ;;
esac
```

**Deliverables:**
- [ ] Automated backup scripts for all databases
- [ ] Point-in-time recovery procedures
- [ ] Backup integrity testing
- [ ] S3 backup storage integration
- [ ] Disaster recovery documentation

---

## Final Deliverables Summary

### 1. PostgreSQL Core Database
- [ ] PostgreSQL 15 with required extensions (pgvector, pg_trgm, uuid-ossp)
- [ ] Comprehensive schema with optimized indexes
- [ ] Alembic migration system with automated management
- [ ] Connection pooling and performance monitoring

### 2. Vector Database Integration
- [ ] Chroma vector database with ClickHouse backend
- [ ] OpenAI embedding integration
- [ ] Batch processing and similarity search
- [ ] User-isolated collections

### 3. Redis Cache & Queue System
- [ ] Redis 7 with optimized configuration
- [ ] Comprehensive caching, session, and queue services
- [ ] Rate limiting and pub/sub capabilities
- [ ] Connection pooling and health monitoring

### 4. Performance Optimization
- [ ] Query optimization utilities
- [ ] Specialized indexes for common patterns
- [ ] Slow query monitoring and alerting
- [ ] Database statistics and health checks

### 5. Backup & Recovery
- [ ] Automated backup system for all databases
- [ ] Point-in-time recovery procedures
- [ ] Backup integrity testing
- [ ] Disaster recovery documentation

### 6. Monitoring & Observability
- [ ] Database performance metrics
- [ ] Connection pool monitoring
- [ ] Query performance tracking
- [ ] Health check endpoints

---

## Success Validation

To validate successful completion of this epic:

1. **Multi-Database Integration**: All three databases (PostgreSQL, Chroma, Redis) working together
2. **Performance Requirements**: Timeline queries < 100ms, vector searches < 500ms
3. **Migration System**: Automated migrations with rollback capability
4. **Backup & Recovery**: Successful backup and restore procedures tested
5. **Monitoring**: All database metrics visible in dashboards
6. **Load Testing**: System handles expected user load and data volume

---

**Epic Owner**: Backend Developer + Database Engineer  
**Stakeholders**: Development Team, DevOps Team, Data Team  
**Dependencies**: 1.1.1 Development Environment & DevOps  
**Risk Level**: Medium-High (complexity of multi-database integration)