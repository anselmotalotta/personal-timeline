# Epic 1.2.1: Facebook Data Ingestion

**Phase**: 1 - Foundation & Core Infrastructure  
**Epic**: 1.2.1 Facebook Data Ingestion  
**Duration**: 2 weeks  
**Team Size**: 2 developers (Backend + Data Processing)  

---

## Goal

Build a comprehensive Facebook data ingestion system that can parse, validate, and import Facebook data exports (JSON format) into the personal timeline database, with intelligent content extraction, media processing, and privacy-aware data handling.

---

## Scope Assumptions

### Facebook Data Format
- **Data Source**: Facebook "Download Your Information" export (JSON format)
- **Content Types**: Posts, photos, videos, check-ins, messages, events, friends
- **File Structure**: Hierarchical JSON files with media assets
- **Size Range**: 100MB to 10GB+ per user export

### Processing Requirements
- **Batch Processing**: Handle large exports efficiently
- **Content Extraction**: Parse text, media, location, people, timestamps
- **Media Processing**: Extract metadata, generate thumbnails, detect faces
- **Privacy Filtering**: Respect user privacy settings and content filtering
- **Error Handling**: Graceful handling of malformed or incomplete data

### Data Mapping
- **Posts** → Memories (with content analysis)
- **Photos/Videos** → MediaFiles (with AI analysis)
- **Check-ins** → Location-based memories
- **Friends** → People entities
- **Events** → Event-based memories

---

## Success Criteria

- [ ] **Complete Data Import**: Successfully import all supported Facebook data types
- [ ] **Content Accuracy**: 95%+ accuracy in content extraction and mapping
- [ ] **Media Processing**: All images/videos processed with metadata extraction
- [ ] **Performance**: Process 1GB of data in < 30 minutes
- [ ] **Error Handling**: Graceful handling of corrupted or incomplete data
- [ ] **Privacy Compliance**: Proper handling of sensitive content and privacy settings
- [ ] **Progress Tracking**: Real-time import progress and status updates
- [ ] **Data Validation**: Comprehensive validation of imported data integrity

---

## Non-Goals

- Real-time Facebook API integration (file-based import only)
- Instagram or other Meta platform data (Facebook only for MVP)
- Advanced content moderation (basic filtering only)
- Automatic friend network reconstruction (individual import only)
- Historical data reconstruction beyond provided exports

---

## Risks & Mitigations

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Facebook export format changes | High | Medium | Version detection, flexible parsing, format documentation |
| Large file processing memory issues | Medium | Medium | Streaming processing, chunked reading, memory monitoring |
| Corrupted or incomplete exports | Medium | High | Robust error handling, partial import capability, validation |
| Privacy violations in content | High | Low | Content filtering, user consent, privacy-first processing |
| Media processing failures | Medium | Medium | Fallback processing, error recovery, manual review queue |

---

## Detailed Tasks

### Task 1: Facebook Export Format Analysis & Parser

#### 1.1 Facebook Data Structure Analysis

**Export Structure Documentation (apps/worker/parsers/facebook/structure.py):**
```python
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import json
import os
from pathlib import Path

class FacebookDataType(Enum):
    """Facebook data types in export"""
    POSTS = "posts"
    PHOTOS = "photos_and_videos"
    MESSAGES = "messages"
    FRIENDS = "friends_and_followers"
    EVENTS = "events"
    PAGES = "pages"
    ADS = "ads_and_businesses"
    APPS = "apps_and_websites"
    COMMENTS = "comments_and_reactions"
    GROUPS = "groups"
    MARKETPLACE = "marketplace"
    PAYMENT = "payment_history"
    SEARCH = "search_history"
    SECURITY = "security_and_login_info"
    LOCATION = "location_history"

@dataclass
class FacebookExportStructure:
    """Facebook export structure definition"""
    
    # Main data directories
    posts_dir: str = "posts"
    photos_dir: str = "photos_and_videos"
    messages_dir: str = "messages"
    friends_dir: str = "friends_and_followers"
    events_dir: str = "events"
    
    # Key files
    posts_file: str = "posts/your_posts_1.json"
    photos_file: str = "photos_and_videos/photos.json"
    videos_file: str = "photos_and_videos/videos.json"
    friends_file: str = "friends_and_followers/friends.json"
    
    # Media directories
    photos_media_dir: str = "photos_and_videos"
    posts_media_dir: str = "posts/media"

class FacebookExportAnalyzer:
    """Analyze Facebook export structure and content"""
    
    def __init__(self, export_path: str):
        self.export_path = Path(export_path)
        self.structure = FacebookExportStructure()
        self.detected_files = {}
        self.export_info = {}
    
    def analyze_export(self) -> Dict[str, Any]:
        """Analyze the Facebook export structure"""
        
        if not self.export_path.exists():
            raise FileNotFoundError(f"Export path not found: {self.export_path}")
        
        analysis = {
            "export_path": str(self.export_path),
            "total_size_bytes": self._calculate_total_size(),
            "detected_data_types": [],
            "file_counts": {},
            "media_counts": {},
            "estimated_processing_time": 0,
            "potential_issues": []
        }
        
        # Detect available data types
        for data_type in FacebookDataType:
            if self._detect_data_type(data_type):
                analysis["detected_data_types"].append(data_type.value)
                analysis["file_counts"][data_type.value] = self._count_files(data_type)
        
        # Analyze media files
        analysis["media_counts"] = self._analyze_media_files()
        
        # Estimate processing time
        analysis["estimated_processing_time"] = self._estimate_processing_time(analysis)
        
        # Detect potential issues
        analysis["potential_issues"] = self._detect_potential_issues()
        
        return analysis
    
    def _detect_data_type(self, data_type: FacebookDataType) -> bool:
        """Detect if a specific data type exists in the export"""
        
        type_paths = {
            FacebookDataType.POSTS: ["posts", "posts/your_posts_1.json"],
            FacebookDataType.PHOTOS: ["photos_and_videos", "photos_and_videos/photos.json"],
            FacebookDataType.MESSAGES: ["messages"],
            FacebookDataType.FRIENDS: ["friends_and_followers", "friends_and_followers/friends.json"],
            FacebookDataType.EVENTS: ["events"],
        }
        
        if data_type not in type_paths:
            return False
        
        for path in type_paths[data_type]:
            full_path = self.export_path / path
            if full_path.exists():
                return True
        
        return False
    
    def _count_files(self, data_type: FacebookDataType) -> int:
        """Count files for a specific data type"""
        
        type_dirs = {
            FacebookDataType.POSTS: "posts",
            FacebookDataType.PHOTOS: "photos_and_videos",
            FacebookDataType.MESSAGES: "messages",
            FacebookDataType.FRIENDS: "friends_and_followers",
            FacebookDataType.EVENTS: "events",
        }
        
        if data_type not in type_dirs:
            return 0
        
        data_dir = self.export_path / type_dirs[data_type]
        if not data_dir.exists():
            return 0
        
        return len([f for f in data_dir.rglob("*.json")])
    
    def _analyze_media_files(self) -> Dict[str, int]:
        """Analyze media files in the export"""
        
        media_counts = {
            "total_images": 0,
            "total_videos": 0,
            "total_size_mb": 0
        }
        
        # Check photos and videos directory
        media_dir = self.export_path / "photos_and_videos"
        if media_dir.exists():
            for file_path in media_dir.rglob("*"):
                if file_path.is_file():
                    suffix = file_path.suffix.lower()
                    if suffix in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                        media_counts["total_images"] += 1
                    elif suffix in ['.mp4', '.mov', '.avi', '.mkv', '.webm']:
                        media_counts["total_videos"] += 1
                    
                    media_counts["total_size_mb"] += file_path.stat().st_size / (1024 * 1024)
        
        return media_counts
    
    def _calculate_total_size(self) -> int:
        """Calculate total size of the export"""
        total_size = 0
        for file_path in self.export_path.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size
    
    def _estimate_processing_time(self, analysis: Dict[str, Any]) -> int:
        """Estimate processing time in minutes"""
        
        # Base time estimates (in minutes)
        base_time = 5  # Setup and initialization
        
        # Time per data type
        type_times = {
            "posts": analysis["file_counts"].get("posts", 0) * 2,
            "photos_and_videos": analysis["media_counts"]["total_images"] * 0.1 + 
                               analysis["media_counts"]["total_videos"] * 0.5,
            "messages": analysis["file_counts"].get("messages", 0) * 1,
            "friends_and_followers": analysis["file_counts"].get("friends_and_followers", 0) * 0.5,
        }
        
        total_time = base_time + sum(type_times.values())
        
        # Add overhead for large exports
        if analysis["total_size_bytes"] > 1024 * 1024 * 1024:  # > 1GB
            total_time *= 1.5
        
        return int(total_time)
    
    def _detect_potential_issues(self) -> List[str]:
        """Detect potential issues with the export"""
        
        issues = []
        
        # Check for very large exports
        if self.export_path.stat().st_size > 5 * 1024 * 1024 * 1024:  # > 5GB
            issues.append("Very large export (>5GB) may require extended processing time")
        
        # Check for missing key files
        key_files = [
            "posts/your_posts_1.json",
            "photos_and_videos/photos.json"
        ]
        
        for key_file in key_files:
            if not (self.export_path / key_file).exists():
                issues.append(f"Missing key file: {key_file}")
        
        # Check for unusual directory structure
        expected_dirs = ["posts", "photos_and_videos", "messages", "friends_and_followers"]
        existing_dirs = [d.name for d in self.export_path.iterdir() if d.is_dir()]
        
        if not any(d in existing_dirs for d in expected_dirs):
            issues.append("Unusual directory structure - may not be a standard Facebook export")
        
        return issues

def detect_facebook_export_version(export_path: str) -> str:
    """Detect Facebook export format version"""
    
    # Check for version indicators in the export
    version_indicators = {
        "2023": ["posts/your_posts_1.json", "photos_and_videos/photos.json"],
        "2022": ["posts/your_posts.json", "photos_and_videos/album"],
        "2021": ["html/posts.html", "photos_and_videos/album"],
    }
    
    export_path = Path(export_path)
    
    for version, indicators in version_indicators.items():
        if all((export_path / indicator).exists() for indicator in indicators):
            return version
    
    # Default to latest known format
    return "2023"
```

**Facebook JSON Parser (apps/worker/parsers/facebook/parser.py):**
```python
import json
import logging
from typing import Dict, List, Optional, Any, Iterator, Tuple
from pathlib import Path
from datetime import datetime
import re
from dataclasses import dataclass
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

@dataclass
class FacebookPost:
    """Parsed Facebook post data"""
    timestamp: datetime
    content: Optional[str]
    attachments: List[Dict[str, Any]]
    location: Optional[Dict[str, Any]]
    people_tagged: List[str]
    reactions: Dict[str, int]
    comments: List[Dict[str, Any]]
    privacy: str
    post_type: str
    original_data: Dict[str, Any]

@dataclass
class FacebookPhoto:
    """Parsed Facebook photo data"""
    timestamp: datetime
    title: Optional[str]
    description: Optional[str]
    file_path: str
    location: Optional[Dict[str, Any]]
    people_tagged: List[str]
    album: Optional[str]
    camera_info: Optional[Dict[str, Any]]
    original_data: Dict[str, Any]

@dataclass
class FacebookFriend:
    """Parsed Facebook friend data"""
    name: str
    timestamp: datetime
    contact_info: Optional[Dict[str, Any]]
    original_data: Dict[str, Any]

class FacebookDataParser:
    """Parse Facebook export data into structured format"""
    
    def __init__(self, export_path: str):
        self.export_path = Path(export_path)
        self.encoding = 'utf-8'
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def parse_posts(self) -> Iterator[FacebookPost]:
        """Parse Facebook posts from export"""
        
        posts_dir = self.export_path / "posts"
        if not posts_dir.exists():
            logger.warning("Posts directory not found")
            return
        
        # Find all post JSON files
        post_files = list(posts_dir.glob("your_posts_*.json"))
        
        if not post_files:
            logger.warning("No post files found")
            return
        
        for post_file in post_files:
            logger.info(f"Parsing posts from {post_file}")
            
            try:
                async for post in self._parse_posts_file(post_file):
                    yield post
            except Exception as e:
                logger.error(f"Error parsing posts file {post_file}: {e}")
                continue
    
    async def _parse_posts_file(self, file_path: Path) -> Iterator[FacebookPost]:
        """Parse a single posts JSON file"""
        
        try:
            # Read file in executor to avoid blocking
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor, 
                self._read_json_file, 
                file_path
            )
            
            if not content or 'posts' not in content:
                logger.warning(f"Invalid posts file format: {file_path}")
                return
            
            for post_data in content['posts']:
                try:
                    post = self._parse_single_post(post_data)
                    if post:
                        yield post
                except Exception as e:
                    logger.error(f"Error parsing individual post: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error reading posts file {file_path}: {e}")
    
    def _parse_single_post(self, post_data: Dict[str, Any]) -> Optional[FacebookPost]:
        """Parse a single post from JSON data"""
        
        try:
            # Extract timestamp
            timestamp = self._parse_timestamp(post_data.get('timestamp'))
            if not timestamp:
                logger.warning("Post missing timestamp, skipping")
                return None
            
            # Extract content
            content = None
            if 'data' in post_data:
                for data_item in post_data['data']:
                    if 'post' in data_item:
                        content = self._decode_facebook_text(data_item['post'])
                        break
            
            # Extract attachments
            attachments = []
            if 'attachments' in post_data:
                attachments = self._parse_attachments(post_data['attachments'])
            
            # Extract location
            location = None
            if 'place' in post_data:
                location = self._parse_location(post_data['place'])
            
            # Extract tagged people
            people_tagged = []
            if 'tags' in post_data:
                people_tagged = [self._decode_facebook_text(tag) for tag in post_data['tags']]
            
            # Extract reactions (if available)
            reactions = {}
            if 'reactions' in post_data:
                reactions = self._parse_reactions(post_data['reactions'])
            
            # Extract comments (if available)
            comments = []
            if 'comments' in post_data:
                comments = self._parse_comments(post_data['comments'])
            
            # Determine post type
            post_type = self._determine_post_type(post_data, attachments)
            
            # Extract privacy setting
            privacy = post_data.get('privacy', 'unknown')
            
            return FacebookPost(
                timestamp=timestamp,
                content=content,
                attachments=attachments,
                location=location,
                people_tagged=people_tagged,
                reactions=reactions,
                comments=comments,
                privacy=privacy,
                post_type=post_type,
                original_data=post_data
            )
            
        except Exception as e:
            logger.error(f"Error parsing post: {e}")
            return None
    
    def _parse_timestamp(self, timestamp_data: Any) -> Optional[datetime]:
        """Parse Facebook timestamp"""
        
        if not timestamp_data:
            return None
        
        try:
            if isinstance(timestamp_data, int):
                return datetime.fromtimestamp(timestamp_data)
            elif isinstance(timestamp_data, str):
                # Try different timestamp formats
                formats = [
                    "%Y-%m-%dT%H:%M:%S%z",
                    "%Y-%m-%d %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S",
                ]
                
                for fmt in formats:
                    try:
                        return datetime.strptime(timestamp_data, fmt)
                    except ValueError:
                        continue
                
                # Try parsing as Unix timestamp string
                try:
                    return datetime.fromtimestamp(int(timestamp_data))
                except (ValueError, TypeError):
                    pass
            
            logger.warning(f"Could not parse timestamp: {timestamp_data}")
            return None
            
        except Exception as e:
            logger.error(f"Error parsing timestamp {timestamp_data}: {e}")
            return None
    
    def _decode_facebook_text(self, text: str) -> str:
        """Decode Facebook's text encoding"""
        
        if not text:
            return ""
        
        try:
            # Facebook often uses Latin-1 encoding that needs to be decoded as UTF-8
            if isinstance(text, str):
                # Try to encode as Latin-1 then decode as UTF-8
                try:
                    return text.encode('latin-1').decode('utf-8')
                except (UnicodeDecodeError, UnicodeEncodeError):
                    return text
            
            return str(text)
            
        except Exception as e:
            logger.warning(f"Error decoding text: {e}")
            return str(text) if text else ""
    
    def _parse_attachments(self, attachments_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Parse post attachments"""
        
        parsed_attachments = []
        
        for attachment in attachments_data:
            try:
                parsed_attachment = {
                    "type": "unknown",
                    "data": {}
                }
                
                # Parse different attachment types
                if 'data' in attachment:
                    for data_item in attachment['data']:
                        if 'media' in data_item:
                            parsed_attachment["type"] = "media"
                            parsed_attachment["data"] = self._parse_media_attachment(data_item['media'])
                        elif 'external_context' in data_item:
                            parsed_attachment["type"] = "link"
                            parsed_attachment["data"] = self._parse_link_attachment(data_item['external_context'])
                        elif 'place' in data_item:
                            parsed_attachment["type"] = "location"
                            parsed_attachment["data"] = self._parse_location(data_item['place'])
                        elif 'event' in data_item:
                            parsed_attachment["type"] = "event"
                            parsed_attachment["data"] = self._parse_event_attachment(data_item['event'])
                
                parsed_attachments.append(parsed_attachment)
                
            except Exception as e:
                logger.error(f"Error parsing attachment: {e}")
                continue
        
        return parsed_attachments
    
    def _parse_media_attachment(self, media_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse media attachment data"""
        
        return {
            "uri": media_data.get('uri', ''),
            "description": self._decode_facebook_text(media_data.get('description', '')),
            "title": self._decode_facebook_text(media_data.get('title', '')),
            "creation_timestamp": self._parse_timestamp(media_data.get('creation_timestamp')),
            "media_metadata": media_data.get('media_metadata', {})
        }
    
    def _parse_link_attachment(self, link_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse link attachment data"""
        
        return {
            "url": link_data.get('url', ''),
            "name": self._decode_facebook_text(link_data.get('name', '')),
            "description": self._decode_facebook_text(link_data.get('description', ''))
        }
    
    def _parse_location(self, place_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse location/place data"""
        
        return {
            "name": self._decode_facebook_text(place_data.get('name', '')),
            "coordinate": place_data.get('coordinate', {}),
            "address": self._decode_facebook_text(place_data.get('address', '')),
            "url": place_data.get('url', '')
        }
    
    def _parse_event_attachment(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse event attachment data"""
        
        return {
            "name": self._decode_facebook_text(event_data.get('name', '')),
            "description": self._decode_facebook_text(event_data.get('description', '')),
            "start_timestamp": self._parse_timestamp(event_data.get('start_timestamp')),
            "end_timestamp": self._parse_timestamp(event_data.get('end_timestamp')),
            "place": self._parse_location(event_data.get('place', {})) if 'place' in event_data else None
        }
    
    def _parse_reactions(self, reactions_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """Parse post reactions"""
        
        reactions = {}
        
        for reaction in reactions_data:
            reaction_type = reaction.get('reaction', 'like')
            actor = self._decode_facebook_text(reaction.get('actor', ''))
            
            if reaction_type not in reactions:
                reactions[reaction_type] = 0
            reactions[reaction_type] += 1
        
        return reactions
    
    def _parse_comments(self, comments_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Parse post comments"""
        
        comments = []
        
        for comment in comments_data:
            try:
                parsed_comment = {
                    "author": self._decode_facebook_text(comment.get('author', '')),
                    "comment": self._decode_facebook_text(comment.get('comment', '')),
                    "timestamp": self._parse_timestamp(comment.get('timestamp'))
                }
                comments.append(parsed_comment)
            except Exception as e:
                logger.error(f"Error parsing comment: {e}")
                continue
        
        return comments
    
    def _determine_post_type(self, post_data: Dict[str, Any], attachments: List[Dict[str, Any]]) -> str:
        """Determine the type of post"""
        
        # Check for specific post types based on content
        if attachments:
            attachment_types = [att.get('type') for att in attachments]
            
            if 'media' in attachment_types:
                return 'photo_video'
            elif 'location' in attachment_types:
                return 'check_in'
            elif 'event' in attachment_types:
                return 'event'
            elif 'link' in attachment_types:
                return 'link_share'
        
        # Check for life events
        if 'life_event' in post_data:
            return 'life_event'
        
        # Default to status update
        return 'status'
    
    def _read_json_file(self, file_path: Path) -> Dict[str, Any]:
        """Read and parse JSON file"""
        
        try:
            with open(file_path, 'r', encoding=self.encoding) as f:
                return json.load(f)
        except UnicodeDecodeError:
            # Try with different encoding
            with open(file_path, 'r', encoding='latin-1') as f:
                return json.load(f)
    
    async def parse_photos(self) -> Iterator[FacebookPhoto]:
        """Parse Facebook photos from export"""
        
        photos_file = self.export_path / "photos_and_videos" / "photos.json"
        
        if not photos_file.exists():
            logger.warning("Photos file not found")
            return
        
        try:
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor,
                self._read_json_file,
                photos_file
            )
            
            if not content or 'photos' not in content:
                logger.warning("Invalid photos file format")
                return
            
            for photo_data in content['photos']:
                try:
                    photo = self._parse_single_photo(photo_data)
                    if photo:
                        yield photo
                except Exception as e:
                    logger.error(f"Error parsing photo: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error parsing photos file: {e}")
    
    def _parse_single_photo(self, photo_data: Dict[str, Any]) -> Optional[FacebookPhoto]:
        """Parse a single photo from JSON data"""
        
        try:
            timestamp = self._parse_timestamp(photo_data.get('creation_timestamp'))
            if not timestamp:
                return None
            
            title = self._decode_facebook_text(photo_data.get('title', ''))
            description = self._decode_facebook_text(photo_data.get('description', ''))
            
            # Get file path
            file_path = photo_data.get('uri', '')
            
            # Parse location if available
            location = None
            if 'place' in photo_data:
                location = self._parse_location(photo_data['place'])
            
            # Parse tagged people
            people_tagged = []
            if 'tags' in photo_data:
                people_tagged = [self._decode_facebook_text(tag) for tag in photo_data['tags']]
            
            # Get album information
            album = None
            if 'album' in photo_data:
                album = self._decode_facebook_text(photo_data['album'])
            
            # Parse camera info
            camera_info = None
            if 'media_metadata' in photo_data:
                camera_info = photo_data['media_metadata']
            
            return FacebookPhoto(
                timestamp=timestamp,
                title=title,
                description=description,
                file_path=file_path,
                location=location,
                people_tagged=people_tagged,
                album=album,
                camera_info=camera_info,
                original_data=photo_data
            )
            
        except Exception as e:
            logger.error(f"Error parsing photo: {e}")
            return None
    
    async def parse_friends(self) -> Iterator[FacebookFriend]:
        """Parse Facebook friends from export"""
        
        friends_file = self.export_path / "friends_and_followers" / "friends.json"
        
        if not friends_file.exists():
            logger.warning("Friends file not found")
            return
        
        try:
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor,
                self._read_json_file,
                friends_file
            )
            
            if not content or 'friends' not in content:
                logger.warning("Invalid friends file format")
                return
            
            for friend_data in content['friends']:
                try:
                    friend = self._parse_single_friend(friend_data)
                    if friend:
                        yield friend
                except Exception as e:
                    logger.error(f"Error parsing friend: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error parsing friends file: {e}")
    
    def _parse_single_friend(self, friend_data: Dict[str, Any]) -> Optional[FacebookFriend]:
        """Parse a single friend from JSON data"""
        
        try:
            name = self._decode_facebook_text(friend_data.get('name', ''))
            if not name:
                return None
            
            timestamp = self._parse_timestamp(friend_data.get('timestamp'))
            if not timestamp:
                timestamp = datetime.now()  # Fallback to current time
            
            # Parse contact info if available
            contact_info = None
            if 'contact_info' in friend_data:
                contact_info = friend_data['contact_info']
            
            return FacebookFriend(
                name=name,
                timestamp=timestamp,
                contact_info=contact_info,
                original_data=friend_data
            )
            
        except Exception as e:
            logger.error(f"Error parsing friend: {e}")
            return None
```

**Deliverables:**
- [ ] Facebook export structure analyzer
- [ ] Comprehensive JSON parser for all data types
- [ ] Text encoding/decoding utilities
- [ ] Robust error handling for malformed data
- [ ] Async processing for large files

#### 1.2 Data Validation & Sanitization

**Data Validation Engine (apps/worker/parsers/facebook/validator.py):**
```python
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import re
from pathlib import Path
from dataclasses import dataclass
import hashlib
import mimetypes

from .parser import FacebookPost, FacebookPhoto, FacebookFriend

logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """Result of data validation"""
    is_valid: bool
    errors: List[str]
    warnings: List[str]
    sanitized_data: Optional[Any] = None

@dataclass
class ValidationStats:
    """Statistics from validation process"""
    total_items: int
    valid_items: int
    invalid_items: int
    warnings_count: int
    errors_by_type: Dict[str, int]

class FacebookDataValidator:
    """Validate and sanitize Facebook data"""
    
    def __init__(self, privacy_settings: Optional[Dict[str, Any]] = None):
        self.privacy_settings = privacy_settings or {}
        self.validation_stats = ValidationStats(
            total_items=0,
            valid_items=0,
            invalid_items=0,
            warnings_count=0,
            errors_by_type={}
        )
        
        # Content filtering patterns
        self.sensitive_patterns = [
            r'\b(?:password|ssn|social security|credit card)\b',
            r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',  # Credit card
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
        ]
        
        # Allowed file extensions
        self.allowed_image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp'}
        self.allowed_video_extensions = {'.mp4', '.mov', '.avi', '.mkv', '.webm', '.m4v'}
        
        # Size limits
        self.max_text_length = 10000
        self.max_file_size_mb = 100
        self.min_timestamp = datetime(2004, 1, 1)  # Facebook founding
        self.max_timestamp = datetime.now() + timedelta(days=1)
    
    def validate_post(self, post: FacebookPost) -> ValidationResult:
        """Validate a Facebook post"""
        
        self.validation_stats.total_items += 1
        errors = []
        warnings = []
        
        try:
            # Validate timestamp
            if not self._validate_timestamp(post.timestamp):
                errors.append(f"Invalid timestamp: {post.timestamp}")
            
            # Validate content
            content_result = self._validate_content(post.content)
            if content_result.errors:
                errors.extend(content_result.errors)
            if content_result.warnings:
                warnings.extend(content_result.warnings)
            
            # Validate attachments
            for i, attachment in enumerate(post.attachments):
                attachment_result = self._validate_attachment(attachment)
                if attachment_result.errors:
                    errors.extend([f"Attachment {i}: {error}" for error in attachment_result.errors])
                if attachment_result.warnings:
                    warnings.extend([f"Attachment {i}: {warning}" for warning in attachment_result.warnings])
            
            # Validate location
            if post.location:
                location_result = self._validate_location(post.location)
                if location_result.errors:
                    errors.extend(location_result.errors)
                if location_result.warnings:
                    warnings.extend(location_result.warnings)
            
            # Validate people tags
            people_result = self._validate_people_tags(post.people_tagged)
            if people_result.errors:
                errors.extend(people_result.errors)
            if people_result.warnings:
                warnings.extend(people_result.warnings)
            
            # Privacy validation
            privacy_result = self._validate_privacy_settings(post.privacy, post.content)
            if privacy_result.errors:
                errors.extend(privacy_result.errors)
            if privacy_result.warnings:
                warnings.extend(privacy_result.warnings)
            
            # Create sanitized version
            sanitized_post = self._sanitize_post(post)
            
            is_valid = len(errors) == 0
            
            if is_valid:
                self.validation_stats.valid_items += 1
            else:
                self.validation_stats.invalid_items += 1
                for error in errors:
                    error_type = error.split(':')[0]
                    self.validation_stats.errors_by_type[error_type] = \
                        self.validation_stats.errors_by_type.get(error_type, 0) + 1
            
            if warnings:
                self.validation_stats.warnings_count += len(warnings)
            
            return ValidationResult(
                is_valid=is_valid,
                errors=errors,
                warnings=warnings,
                sanitized_data=sanitized_post if is_valid else None
            )
            
        except Exception as e:
            logger.error(f"Error validating post: {e}")
            self.validation_stats.invalid_items += 1
            return ValidationResult(
                is_valid=False,
                errors=[f"Validation error: {str(e)}"],
                warnings=[],
                sanitized_data=None
            )
    
    def validate_photo(self, photo: FacebookPhoto, export_path: str) -> ValidationResult:
        """Validate a Facebook photo"""
        
        self.validation_stats.total_items += 1
        errors = []
        warnings = []
        
        try:
            # Validate timestamp
            if not self._validate_timestamp(photo.timestamp):
                errors.append(f"Invalid timestamp: {photo.timestamp}")
            
            # Validate file path and existence
            file_result = self._validate_media_file(photo.file_path, export_path)
            if file_result.errors:
                errors.extend(file_result.errors)
            if file_result.warnings:
                warnings.extend(file_result.warnings)
            
            # Validate content
            if photo.title:
                title_result = self._validate_content(photo.title)
                if title_result.errors:
                    errors.extend([f"Title: {error}" for error in title_result.errors])
                if title_result.warnings:
                    warnings.extend([f"Title: {warning}" for warning in title_result.warnings])
            
            if photo.description:
                desc_result = self._validate_content(photo.description)
                if desc_result.errors:
                    errors.extend([f"Description: {error}" for error in desc_result.errors])
                if desc_result.warnings:
                    warnings.extend([f"Description: {warning}" for warning in desc_result.warnings])
            
            # Validate location
            if photo.location:
                location_result = self._validate_location(photo.location)
                if location_result.errors:
                    errors.extend(location_result.errors)
                if location_result.warnings:
                    warnings.extend(location_result.warnings)
            
            # Validate people tags
            people_result = self._validate_people_tags(photo.people_tagged)
            if people_result.errors:
                errors.extend(people_result.errors)
            if people_result.warnings:
                warnings.extend(people_result.warnings)
            
            # Create sanitized version
            sanitized_photo = self._sanitize_photo(photo)
            
            is_valid = len(errors) == 0
            
            if is_valid:
                self.validation_stats.valid_items += 1
            else:
                self.validation_stats.invalid_items += 1
            
            if warnings:
                self.validation_stats.warnings_count += len(warnings)
            
            return ValidationResult(
                is_valid=is_valid,
                errors=errors,
                warnings=warnings,
                sanitized_data=sanitized_photo if is_valid else None
            )
            
        except Exception as e:
            logger.error(f"Error validating photo: {e}")
            self.validation_stats.invalid_items += 1
            return ValidationResult(
                is_valid=False,
                errors=[f"Validation error: {str(e)}"],
                warnings=[],
                sanitized_data=None
            )
    
    def validate_friend(self, friend: FacebookFriend) -> ValidationResult:
        """Validate a Facebook friend"""
        
        self.validation_stats.total_items += 1
        errors = []
        warnings = []
        
        try:
            # Validate name
            if not friend.name or len(friend.name.strip()) == 0:
                errors.append("Friend name is empty")
            elif len(friend.name) > 100:
                warnings.append("Friend name is unusually long")
            
            # Validate timestamp
            if not self._validate_timestamp(friend.timestamp):
                errors.append(f"Invalid timestamp: {friend.timestamp}")
            
            # Validate contact info
            if friend.contact_info:
                contact_result = self._validate_contact_info(friend.contact_info)
                if contact_result.errors:
                    errors.extend(contact_result.errors)
                if contact_result.warnings:
                    warnings.extend(contact_result.warnings)
            
            # Create sanitized version
            sanitized_friend = self._sanitize_friend(friend)
            
            is_valid = len(errors) == 0
            
            if is_valid:
                self.validation_stats.valid_items += 1
            else:
                self.validation_stats.invalid_items += 1
            
            if warnings:
                self.validation_stats.warnings_count += len(warnings)
            
            return ValidationResult(
                is_valid=is_valid,
                errors=errors,
                warnings=warnings,
                sanitized_data=sanitized_friend if is_valid else None
            )
            
        except Exception as e:
            logger.error(f"Error validating friend: {e}")
            self.validation_stats.invalid_items += 1
            return ValidationResult(
                is_valid=False,
                errors=[f"Validation error: {str(e)}"],
                warnings=[],
                sanitized_data=None
            )
    
    def _validate_timestamp(self, timestamp: datetime) -> bool:
        """Validate timestamp is within reasonable bounds"""
        
        if not timestamp:
            return False
        
        return self.min_timestamp <= timestamp <= self.max_timestamp
    
    def _validate_content(self, content: Optional[str]) -> ValidationResult:
        """Validate text content"""
        
        errors = []
        warnings = []
        
        if not content:
            return ValidationResult(True, [], [])
        
        # Check length
        if len(content) > self.max_text_length:
            errors.append(f"Content too long: {len(content)} > {self.max_text_length}")
        
        # Check for sensitive information
        for pattern in self.sensitive_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                warnings.append(f"Potentially sensitive information detected")
                break
        
        # Check for unusual characters
        if any(ord(char) > 127 for char in content):
            # Contains non-ASCII characters - might need special handling
            warnings.append("Content contains non-ASCII characters")
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_attachment(self, attachment: Dict[str, Any]) -> ValidationResult:
        """Validate post attachment"""
        
        errors = []
        warnings = []
        
        if not attachment.get('type'):
            errors.append("Attachment missing type")
            return ValidationResult(False, errors, warnings)
        
        attachment_type = attachment['type']
        attachment_data = attachment.get('data', {})
        
        if attachment_type == 'media':
            # Validate media attachment
            if 'uri' in attachment_data:
                file_path = attachment_data['uri']
                if not self._is_valid_media_file(file_path):
                    warnings.append(f"Invalid media file: {file_path}")
        
        elif attachment_type == 'link':
            # Validate link attachment
            if 'url' in attachment_data:
                url = attachment_data['url']
                if not self._is_valid_url(url):
                    warnings.append(f"Invalid URL: {url}")
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_location(self, location: Dict[str, Any]) -> ValidationResult:
        """Validate location data"""
        
        errors = []
        warnings = []
        
        if 'coordinate' in location:
            coord = location['coordinate']
            if 'latitude' in coord and 'longitude' in coord:
                lat = coord['latitude']
                lng = coord['longitude']
                
                if not (-90 <= lat <= 90):
                    errors.append(f"Invalid latitude: {lat}")
                
                if not (-180 <= lng <= 180):
                    errors.append(f"Invalid longitude: {lng}")
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_people_tags(self, people: List[str]) -> ValidationResult:
        """Validate people tags"""
        
        errors = []
        warnings = []
        
        if len(people) > 50:
            warnings.append(f"Unusually high number of people tagged: {len(people)}")
        
        for person in people:
            if not person or len(person.strip()) == 0:
                warnings.append("Empty person name in tags")
            elif len(person) > 100:
                warnings.append(f"Unusually long person name: {person[:50]}...")
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_privacy_settings(self, privacy: str, content: Optional[str]) -> ValidationResult:
        """Validate privacy settings and content appropriateness"""
        
        errors = []
        warnings = []
        
        # Check if user has privacy restrictions
        if self.privacy_settings.get('strict_privacy', False):
            if privacy not in ['SELF', 'FRIENDS']:
                warnings.append(f"Content with privacy '{privacy}' may not meet user's privacy preferences")
        
        # Check for potentially private content
        if content and self.privacy_settings.get('filter_sensitive', True):
            private_keywords = ['home address', 'phone number', 'personal', 'private']
            if any(keyword in content.lower() for keyword in private_keywords):
                warnings.append("Content may contain private information")
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_media_file(self, file_path: str, export_path: str) -> ValidationResult:
        """Validate media file"""
        
        errors = []
        warnings = []
        
        if not file_path:
            errors.append("Media file path is empty")
            return ValidationResult(False, errors, warnings)
        
        # Check file extension
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.allowed_image_extensions and file_ext not in self.allowed_video_extensions:
            warnings.append(f"Unsupported file extension: {file_ext}")
        
        # Check if file exists in export
        full_path = Path(export_path) / file_path
        if not full_path.exists():
            errors.append(f"Media file not found: {file_path}")
        else:
            # Check file size
            file_size_mb = full_path.stat().st_size / (1024 * 1024)
            if file_size_mb > self.max_file_size_mb:
                warnings.append(f"Large media file: {file_size_mb:.1f}MB")
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_contact_info(self, contact_info: Dict[str, Any]) -> ValidationResult:
        """Validate contact information"""
        
        errors = []
        warnings = []
        
        # Check for sensitive information in contact data
        for key, value in contact_info.items():
            if isinstance(value, str):
                for pattern in self.sensitive_patterns:
                    if re.search(pattern, value, re.IGNORECASE):
                        warnings.append(f"Potentially sensitive information in contact {key}")
                        break
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
    
    def _is_valid_media_file(self, file_path: str) -> bool:
        """Check if file path represents a valid media file"""
        
        if not file_path:
            return False
        
        file_ext = Path(file_path).suffix.lower()
        return file_ext in self.allowed_image_extensions or file_ext in self.allowed_video_extensions
    
    def _is_valid_url(self, url: str) -> bool:
        """Check if URL is valid"""
        
        if not url:
            return False
        
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        
        return url_pattern.match(url) is not None
    
    def _sanitize_post(self, post: FacebookPost) -> FacebookPost:
        """Create sanitized version of post"""
        
        # Create a copy with sanitized content
        sanitized_content = self._sanitize_text(post.content) if post.content else None
        
        # Sanitize people tags
        sanitized_people = [self._sanitize_text(person) for person in post.people_tagged if person]
        
        # Create new post object with sanitized data
        return FacebookPost(
            timestamp=post.timestamp,
            content=sanitized_content,
            attachments=post.attachments,  # Attachments are validated separately
            location=post.location,
            people_tagged=sanitized_people,
            reactions=post.reactions,
            comments=post.comments,
            privacy=post.privacy,
            post_type=post.post_type,
            original_data=post.original_data
        )
    
    def _sanitize_photo(self, photo: FacebookPhoto) -> FacebookPhoto:
        """Create sanitized version of photo"""
        
        sanitized_title = self._sanitize_text(photo.title) if photo.title else None
        sanitized_description = self._sanitize_text(photo.description) if photo.description else None
        sanitized_people = [self._sanitize_text(person) for person in photo.people_tagged if person]
        
        return FacebookPhoto(
            timestamp=photo.timestamp,
            title=sanitized_title,
            description=sanitized_description,
            file_path=photo.file_path,
            location=photo.location,
            people_tagged=sanitized_people,
            album=photo.album,
            camera_info=photo.camera_info,
            original_data=photo.original_data
        )
    
    def _sanitize_friend(self, friend: FacebookFriend) -> FacebookFriend:
        """Create sanitized version of friend"""
        
        sanitized_name = self._sanitize_text(friend.name)
        
        return FacebookFriend(
            name=sanitized_name,
            timestamp=friend.timestamp,
            contact_info=friend.contact_info,  # Contact info handled separately
            original_data=friend.original_data
        )
    
    def _sanitize_text(self, text: str) -> str:
        """Sanitize text content"""
        
        if not text:
            return ""
        
        # Remove or mask sensitive patterns
        sanitized = text
        
        for pattern in self.sensitive_patterns:
            sanitized = re.sub(pattern, '[REDACTED]', sanitized, flags=re.IGNORECASE)
        
        # Normalize whitespace
        sanitized = ' '.join(sanitized.split())
        
        # Truncate if too long
        if len(sanitized) > self.max_text_length:
            sanitized = sanitized[:self.max_text_length] + '...'
        
        return sanitized
    
    def get_validation_summary(self) -> Dict[str, Any]:
        """Get summary of validation results"""
        
        success_rate = (self.validation_stats.valid_items / self.validation_stats.total_items * 100) \
            if self.validation_stats.total_items > 0 else 0
        
        return {
            "total_items": self.validation_stats.total_items,
            "valid_items": self.validation_stats.valid_items,
            "invalid_items": self.validation_stats.invalid_items,
            "success_rate": round(success_rate, 2),
            "warnings_count": self.validation_stats.warnings_count,
            "errors_by_type": dict(self.validation_stats.errors_by_type),
            "most_common_errors": sorted(
                self.validation_stats.errors_by_type.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
        }
```

**Deliverables:**
- [ ] Comprehensive data validation engine
- [ ] Content sanitization and privacy filtering
- [ ] Media file validation and verification
- [ ] Validation statistics and reporting
- [ ] Configurable privacy settings

### Task 2: Database Integration & Mapping

#### 2.1 Facebook Data to Database Mapping

**Data Mapping Service (apps/worker/services/facebook_mapper.py):**
```python
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from uuid import uuid4
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.orm import selectinload

from models.database import (
    User, Memory, MediaFile, Person, MemoryPerson, 
    Place, DataSource
)
from parsers.facebook.parser import FacebookPost, FacebookPhoto, FacebookFriend
from services.vector_service import vector_service
from utils.media_processor import MediaProcessor

logger = logging.getLogger(__name__)

class FacebookDataMapper:
    """Map Facebook data to database entities"""
    
    def __init__(self, user_id: str, data_source_id: str):
        self.user_id = user_id
        self.data_source_id = data_source_id
        self.media_processor = MediaProcessor()
        
        # Caches for avoiding duplicate lookups
        self.people_cache = {}
        self.places_cache = {}
        
        # Statistics
        self.mapping_stats = {
            "posts_mapped": 0,
            "photos_mapped": 0,
            "friends_mapped": 0,
            "media_files_created": 0,
            "people_created": 0,
            "places_created": 0,
            "errors": 0
        }
    
    async def map_post_to_memory(
        self, 
        post: FacebookPost, 
        db: AsyncSession,
        export_path: str
    ) -> Optional[Memory]:
        """Map Facebook post to Memory entity"""
        
        try:
            # Create base memory
            memory = Memory(
                id=uuid4(),
                user_id=self.user_id,
                data_source_id=self.data_source_id,
                timestamp=post.timestamp,
                content_type=self._map_post_type(post.post_type),
                title=self._generate_memory_title(post),
                description=post.content,
                original_content=self._create_original_content(post),
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            
            # Extract and set location
            if post.location:
                location_data = await self._map_location(post.location, db)
                if location_data:
                    memory.latitude = location_data.get('latitude')
                    memory.longitude = location_data.get('longitude')
                    memory.location_name = location_data.get('name')
                    memory.location_address = location_data.get('address')
            
            # Add to database to get ID
            db.add(memory)
            await db.flush()  # Get the ID without committing
            
            # Process media attachments
            media_files = await self._process_post_media(
                post, memory.id, db, export_path
            )
            
            # Process people tags
            await self._process_people_tags(
                post.people_tagged, memory.id, db
            )
            
            # Generate AI summary and tags (async)
            await self._generate_ai_content(memory, post)
            
            self.mapping_stats["posts_mapped"] += 1
            logger.debug(f"Mapped post to memory: {memory.id}")
            
            return memory
            
        except Exception as e:
            logger.error(f"Error mapping post to memory: {e}")
            self.mapping_stats["errors"] += 1
            return None
    
    async def map_photo_to_memory(
        self, 
        photo: FacebookPhoto, 
        db: AsyncSession,
        export_path: str
    ) -> Optional[Memory]:
        """Map Facebook photo to Memory entity"""
        
        try:
            # Create memory for photo
            memory = Memory(
                id=uuid4(),
                user_id=self.user_id,
                data_source_id=self.data_source_id,
                timestamp=photo.timestamp,
                content_type="photo",
                title=photo.title or "Photo",
                description=photo.description,
                original_content=self._create_photo_original_content(photo),
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            
            # Extract and set location
            if photo.location:
                location_data = await self._map_location(photo.location, db)
                if location_data:
                    memory.latitude = location_data.get('latitude')
                    memory.longitude = location_data.get('longitude')
                    memory.location_name = location_data.get('name')
                    memory.location_address = location_data.get('address')
            
            # Add to database to get ID
            db.add(memory)
            await db.flush()
            
            # Process the photo file
            media_file = await self._process_photo_file(
                photo, memory.id, db, export_path
            )
            
            if media_file:
                self.mapping_stats["media_files_created"] += 1
            
            # Process people tags
            await self._process_people_tags(
                photo.people_tagged, memory.id, db
            )
            
            # Generate AI content for photo
            await self._generate_ai_content_for_photo(memory, photo, media_file)
            
            self.mapping_stats["photos_mapped"] += 1
            logger.debug(f"Mapped photo to memory: {memory.id}")
            
            return memory
            
        except Exception as e:
            logger.error(f"Error mapping photo to memory: {e}")
            self.mapping_stats["errors"] += 1
            return None
    
    async def map_friend_to_person(
        self, 
        friend: FacebookFriend, 
        db: AsyncSession
    ) -> Optional[Person]:
        """Map Facebook friend to Person entity"""
        
        try:
            # Check if person already exists
            if friend.name in self.people_cache:
                return self.people_cache[friend.name]
            
            # Check database
            existing_person = await db.execute(
                select(Person).where(
                    Person.user_id == self.user_id,
                    Person.name == friend.name
                )
            )
            existing_person = existing_person.scalar_one_or_none()
            
            if existing_person:
                self.people_cache[friend.name] = existing_person
                return existing_person
            
            # Create new person
            person = Person(
                id=uuid4(),
                user_id=self.user_id,
                name=friend.name,
                relationship_type="friend",
                first_appearance=friend.timestamp,
                last_appearance=friend.timestamp,
                interaction_count=1,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            
            db.add(person)
            await db.flush()
            
            # Cache the person
            self.people_cache[friend.name] = person
            
            self.mapping_stats["friends_mapped"] += 1
            self.mapping_stats["people_created"] += 1
            
            logger.debug(f"Mapped friend to person: {person.id}")
            
            return person
            
        except Exception as e:
            logger.error(f"Error mapping friend to person: {e}")
            self.mapping_stats["errors"] += 1
            return None
    
    def _map_post_type(self, facebook_post_type: str) -> str:
        """Map Facebook post type to our content type"""
        
        type_mapping = {
            "status": "post",
            "photo_video": "photo",
            "check_in": "check_in",
            "event": "event",
            "link_share": "post",
            "life_event": "post"
        }
        
        return type_mapping.get(facebook_post_type, "post")
    
    def _generate_memory_title(self, post: FacebookPost) -> str:
        """Generate a title for the memory based on post content"""
        
        # If post has content, use first line or sentence
        if post.content:
            # Take first sentence or first 50 characters
            first_sentence = post.content.split('.')[0].strip()
            if len(first_sentence) <= 50:
                return first_sentence
            else:
                return post.content[:50].strip() + "..."
        
        # Generate title based on post type
        if post.post_type == "photo_video":
            return "Photo/Video Post"
        elif post.post_type == "check_in":
            if post.location:
                location_name = post.location.get('name', 'Unknown Location')
                return f"Check-in at {location_name}"
            return "Check-in"
        elif post.post_type == "event":
            return "Event Post"
        elif post.post_type == "life_event":
            return "Life Event"
        else:
            return f"Post from {post.timestamp.strftime('%B %d, %Y')}"
    
    def _create_original_content(self, post: FacebookPost) -> Dict[str, Any]:
        """Create original content structure for post"""
        
        return {
            "type": "facebook_post",
            "post_type": post.post_type,
            "content": post.content,
            "privacy": post.privacy,
            "attachments": post.attachments,
            "reactions": post.reactions,
            "comments": post.comments[:5] if post.comments else [],  # Limit comments
            "people_tagged": post.people_tagged,
            "location": post.location,
            "facebook_data": {
                "timestamp": post.timestamp.isoformat(),
                "has_attachments": len(post.attachments) > 0,
                "reaction_count": sum(post.reactions.values()) if post.reactions else 0,
                "comment_count": len(post.comments) if post.comments else 0
            }
        }
    
    def _create_photo_original_content(self, photo: FacebookPhoto) -> Dict[str, Any]:
        """Create original content structure for photo"""
        
        return {
            "type": "facebook_photo",
            "title": photo.title,
            "description": photo.description,
            "file_path": photo.file_path,
            "album": photo.album,
            "people_tagged": photo.people_tagged,
            "location": photo.location,
            "camera_info": photo.camera_info,
            "facebook_data": {
                "timestamp": photo.timestamp.isoformat(),
                "has_location": photo.location is not None,
                "people_count": len(photo.people_tagged) if photo.people_tagged else 0
            }
        }
    
    async def _map_location(self, location_data: Dict[str, Any], db: AsyncSession) -> Optional[Dict[str, Any]]:
        """Map Facebook location to our location format"""
        
        try:
            location_name = location_data.get('name', '')
            coordinates = location_data.get('coordinate', {})
            
            if not coordinates:
                return None
            
            latitude = coordinates.get('latitude')
            longitude = coordinates.get('longitude')
            
            if latitude is None or longitude is None:
                return None
            
            # Check if we've seen this place before
            place_key = f"{latitude},{longitude}"
            if place_key in self.places_cache:
                place = self.places_cache[place_key]
                return {
                    'latitude': place.latitude,
                    'longitude': place.longitude,
                    'name': place.name,
                    'address': place.address
                }
            
            # Create or find place
            place = await self._get_or_create_place(
                name=location_name,
                latitude=float(latitude),
                longitude=float(longitude),
                address=location_data.get('address', ''),
                db=db
            )
            
            if place:
                self.places_cache[place_key] = place
                return {
                    'latitude': place.latitude,
                    'longitude': place.longitude,
                    'name': place.name,
                    'address': place.address
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Error mapping location: {e}")
            return None
    
    async def _get_or_create_place(
        self, 
        name: str, 
        latitude: float, 
        longitude: float, 
        address: str,
        db: AsyncSession
    ) -> Optional[Place]:
        """Get existing place or create new one"""
        
        try:
            # Look for existing place within small radius (100m)
            existing_place = await db.execute(
                select(Place).where(
                    Place.user_id == self.user_id,
                    Place.latitude.between(latitude - 0.001, latitude + 0.001),
                    Place.longitude.between(longitude - 0.001, longitude + 0.001)
                )
            )
            existing_place = existing_place.scalar_one_or_none()
            
            if existing_place:
                # Update visit count
                existing_place.visit_count += 1
                existing_place.last_visit = datetime.utcnow()
                return existing_place
            
            # Create new place
            place = Place(
                id=uuid4(),
                user_id=self.user_id,
                name=name,
                address=address,
                latitude=latitude,
                longitude=longitude,
                visit_count=1,
                first_visit=datetime.utcnow(),
                last_visit=datetime.utcnow(),
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            
            db.add(place)
            await db.flush()
            
            self.mapping_stats["places_created"] += 1
            
            return place
            
        except Exception as e:
            logger.error(f"Error creating place: {e}")
            return None
    
    async def _process_post_media(
        self, 
        post: FacebookPost, 
        memory_id: str, 
        db: AsyncSession,
        export_path: str
    ) -> List[MediaFile]:
        """Process media attachments from post"""
        
        media_files = []
        
        for attachment in post.attachments:
            if attachment.get('type') == 'media':
                media_data = attachment.get('data', {})
                file_uri = media_data.get('uri', '')
                
                if file_uri:
                    media_file = await self._create_media_file(
                        file_uri, memory_id, db, export_path, media_data
                    )
                    if media_file:
                        media_files.append(media_file)
        
        return media_files
    
    async def _process_photo_file(
        self, 
        photo: FacebookPhoto, 
        memory_id: str, 
        db: AsyncSession,
        export_path: str
    ) -> Optional[MediaFile]:
        """Process a single photo file"""
        
        if not photo.file_path:
            return None
        
        return await self._create_media_file(
            photo.file_path, memory_id, db, export_path, 
            {"title": photo.title, "description": photo.description}
        )
    
    async def _create_media_file(
        self, 
        file_path: str, 
        memory_id: str, 
        db: AsyncSession,
        export_path: str,
        metadata: Dict[str, Any]
    ) -> Optional[MediaFile]:
        """Create MediaFile entity from file path"""
        
        try:
            from pathlib import Path
            import mimetypes
            
            # Get full file path
            full_path = Path(export_path) / file_path
            
            if not full_path.exists():
                logger.warning(f"Media file not found: {file_path}")
                return None
            
            # Determine file type
            mime_type, _ = mimetypes.guess_type(str(full_path))
            file_type = "image" if mime_type and mime_type.startswith("image") else "video"
            
            # Process the media file
            processed_info = await self.media_processor.process_media_file(
                str(full_path), file_type
            )
            
            # Create MediaFile entity
            media_file = MediaFile(
                id=uuid4(),
                memory_id=memory_id,
                file_type=file_type,
                original_filename=full_path.name,
                file_path=processed_info.get('processed_path', str(full_path)),
                thumbnail_path=processed_info.get('thumbnail_path'),
                file_size=full_path.stat().st_size,
                mime_type=mime_type,
                dimensions=processed_info.get('dimensions'),
                duration=processed_info.get('duration'),
                exif_data=processed_info.get('exif_data'),
                ai_analysis=processed_info.get('ai_analysis', {}),
                created_at=datetime.utcnow()
            )
            
            db.add(media_file)
            await db.flush()
            
            logger.debug(f"Created media file: {media_file.id}")
            
            return media_file
            
        except Exception as e:
            logger.error(f"Error creating media file for {file_path}: {e}")
            return None
    
    async def _process_people_tags(
        self, 
        people_tagged: List[str], 
        memory_id: str, 
        db: AsyncSession
    ):
        """Process people tags and create MemoryPerson associations"""
        
        for person_name in people_tagged:
            if not person_name:
                continue
            
            try:
                # Get or create person
                person = await self._get_or_create_person(person_name, db)
                
                if person:
                    # Create MemoryPerson association
                    memory_person = MemoryPerson(
                        id=uuid4(),
                        memory_id=memory_id,
                        person_id=person.id,
                        mention_type="tagged",
                        confidence_score=1.0  # Tagged by user, so high confidence
                    )
                    
                    db.add(memory_person)
                    
                    # Update person statistics
                    person.interaction_count += 1
                    person.last_appearance = datetime.utcnow()
                    
            except Exception as e:
                logger.error(f"Error processing person tag {person_name}: {e}")
                continue
    
    async def _get_or_create_person(self, name: str, db: AsyncSession) -> Optional[Person]:
        """Get existing person or create new one"""
        
        # Check cache first
        if name in self.people_cache:
            return self.people_cache[name]
        
        try:
            # Check database
            existing_person = await db.execute(
                select(Person).where(
                    Person.user_id == self.user_id,
                    Person.name == name
                )
            )
            existing_person = existing_person.scalar_one_or_none()
            
            if existing_person:
                self.people_cache[name] = existing_person
                return existing_person
            
            # Create new person
            person = Person(
                id=uuid4(),
                user_id=self.user_id,
                name=name,
                first_appearance=datetime.utcnow(),
                last_appearance=datetime.utcnow(),
                interaction_count=0,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            
            db.add(person)
            await db.flush()
            
            # Cache the person
            self.people_cache[name] = person
            
            self.mapping_stats["people_created"] += 1
            
            return person
            
        except Exception as e:
            logger.error(f"Error getting/creating person {name}: {e}")
            return None
    
    async def _generate_ai_content(self, memory: Memory, post: FacebookPost):
        """Generate AI summary and tags for memory"""
        
        try:
            # Combine all text content
            text_content = []
            
            if post.content:
                text_content.append(post.content)
            
            if post.location and post.location.get('name'):
                text_content.append(f"Location: {post.location['name']}")
            
            if post.people_tagged:
                text_content.append(f"With: {', '.join(post.people_tagged)}")
            
            # Add attachment descriptions
            for attachment in post.attachments:
                if attachment.get('type') == 'media':
                    media_data = attachment.get('data', {})
                    if media_data.get('description'):
                        text_content.append(media_data['description'])
            
            full_text = ' '.join(text_content)
            
            if full_text.strip():
                # Generate AI summary (placeholder - implement with actual AI service)
                memory.ai_summary = await self._generate_summary(full_text)
                
                # Generate tags (placeholder - implement with actual AI service)
                memory.ai_tags = await self._generate_tags(full_text, post)
                
                # Calculate sentiment (placeholder - implement with actual AI service)
                memory.sentiment_score = await self._calculate_sentiment(full_text)
                
                # Generate embedding for vector search
                await vector_service.add_memory_embedding(
                    self.user_id, memory
                )
            
        except Exception as e:
            logger.error(f"Error generating AI content: {e}")
    
    async def _generate_ai_content_for_photo(
        self, 
        memory: Memory, 
        photo: FacebookPhoto, 
        media_file: Optional[MediaFile]
    ):
        """Generate AI content specifically for photos"""
        
        try:
            # Combine text content
            text_content = []
            
            if photo.title:
                text_content.append(photo.title)
            
            if photo.description:
                text_content.append(photo.description)
            
            if photo.location and photo.location.get('name'):
                text_content.append(f"Location: {photo.location['name']}")
            
            if photo.people_tagged:
                text_content.append(f"People: {', '.join(photo.people_tagged)}")
            
            full_text = ' '.join(text_content)
            
            # Generate AI content
            if full_text.strip():
                memory.ai_summary = await self._generate_summary(full_text)
                memory.ai_tags = await self._generate_photo_tags(photo, media_file)
                memory.sentiment_score = await self._calculate_sentiment(full_text)
                
                # Generate embedding
                await vector_service.add_memory_embedding(
                    self.user_id, memory
                )
            
        except Exception as e:
            logger.error(f"Error generating AI content for photo: {e}")
    
    async def _generate_summary(self, text: str) -> str:
        """Generate AI summary (placeholder)"""
        # TODO: Implement with actual AI service
        if len(text) <= 100:
            return text
        return text[:100] + "..."
    
    async def _generate_tags(self, text: str, post: FacebookPost) -> List[str]:
        """Generate AI tags (placeholder)"""
        # TODO: Implement with actual AI service
        tags = []
        
        # Add basic tags based on post type
        if post.post_type == "photo_video":
            tags.append("photo")
        elif post.post_type == "check_in":
            tags.append("location")
        elif post.post_type == "event":
            tags.append("event")
        
        # Add tags based on content
        if "birthday" in text.lower():
            tags.append("birthday")
        if "vacation" in text.lower() or "trip" in text.lower():
            tags.append("travel")
        if "work" in text.lower():
            tags.append("work")
        
        return tags
    
    async def _generate_photo_tags(self, photo: FacebookPhoto, media_file: Optional[MediaFile]) -> List[str]:
        """Generate tags for photos"""
        # TODO: Implement with actual AI service
        tags = ["photo"]
        
        if photo.album:
            tags.append(f"album:{photo.album.lower()}")
        
        if photo.people_tagged:
            tags.append("people")
        
        if photo.location:
            tags.append("location")
        
        return tags
    
    async def _calculate_sentiment(self, text: str) -> float:
        """Calculate sentiment score (placeholder)"""
        # TODO: Implement with actual AI service
        # Return neutral sentiment for now
        return 0.0
    
    def get_mapping_stats(self) -> Dict[str, Any]:
        """Get mapping statistics"""
        return dict(self.mapping_stats)
```

**Deliverables:**
- [ ] Comprehensive data mapping service
- [ ] Facebook-to-database entity conversion
- [ ] People and places management with deduplication
- [ ] Media file processing and storage
- [ ] AI content generation integration
- [ ] Mapping statistics and monitoring

### Task 3: Import Processing Pipeline

#### 3.1 Asynchronous Import Worker

**Facebook Import Worker (apps/worker/workers/facebook_import_worker.py):**
```python
import asyncio
import logging
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import json
import traceback
from uuid import uuid4

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update

from core.database import AsyncSessionLocal
from models.database import DataSource, User
from parsers.facebook.structure import FacebookExportAnalyzer
from parsers.facebook.parser import FacebookDataParser
from parsers.facebook.validator import FacebookDataValidator
from services.facebook_mapper import FacebookDataMapper
from services.redis_service import redis_service
from utils.file_manager import FileManager

logger = logging.getLogger(__name__)

class FacebookImportWorker:
    """Worker for processing Facebook data imports"""
    
    def __init__(self):
        self.file_manager = FileManager()
        self.current_task = None
        self.is_running = False
    
    async def process_import(
        self, 
        user_id: str, 
        data_source_id: str, 
        export_path: str,
        privacy_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Process Facebook data import"""
        
        self.current_task = {
            "user_id": user_id,
            "data_source_id": data_source_id,
            "export_path": export_path,
            "started_at": datetime.utcnow(),
            "status": "starting"
        }
        
        self.is_running = True
        
        try:
            async with AsyncSessionLocal() as db:
                # Update data source status
                await self._update_data_source_status(
                    data_source_id, "processing", db
                )
                
                # Analyze export structure
                logger.info(f"Analyzing Facebook export: {export_path}")
                analyzer = FacebookExportAnalyzer(export_path)
                analysis = analyzer.analyze_export()
                
                # Store analysis results
                await self._store_analysis_results(data_source_id, analysis, db)
                
                # Initialize components
                parser = FacebookDataParser(export_path)
                validator = FacebookDataValidator(privacy_settings)
                mapper = FacebookDataMapper(user_id, data_source_id)
                
                # Process different data types
                results = {
                    "analysis": analysis,
                    "posts_processed": 0,
                    "photos_processed": 0,
                    "friends_processed": 0,
                    "errors": [],
                    "warnings": [],
                    "processing_time": 0
                }
                
                start_time = datetime.utcnow()
                
                # Process posts
                if "posts" in analysis["detected_data_types"]:
                    logger.info("Processing Facebook posts...")
                    post_results = await self._process_posts(
                        parser, validator, mapper, db, export_path
                    )
                    results.update(post_results)
                
                # Process photos
                if "photos_and_videos" in analysis["detected_data_types"]:
                    logger.info("Processing Facebook photos...")
                    photo_results = await self._process_photos(
                        parser, validator, mapper, db, export_path
                    )
                    results["photos_processed"] = photo_results["photos_processed"]
                    results["errors"].extend(photo_results.get("errors", []))
                    results["warnings"].extend(photo_results.get("warnings", []))
                
                # Process friends
                if "friends_and_followers" in analysis["detected_data_types"]:
                    logger.info("Processing Facebook friends...")
                    friend_results = await self._process_friends(
                        parser, validator, mapper, db
                    )
                    results["friends_processed"] = friend_results["friends_processed"]
                    results["errors"].extend(friend_results.get("errors", []))
                    results["warnings"].extend(friend_results.get("warnings", []))
                
                # Calculate processing time
                end_time = datetime.utcnow()
                results["processing_time"] = (end_time - start_time).total_seconds()
                
                # Update final statistics
                await self._update_final_statistics(
                    data_source_id, results, mapper.get_mapping_stats(), db
                )
                
                # Mark as completed
                await self._update_data_source_status(
                    data_source_id, "completed", db
                )
                
                await db.commit()
                
                logger.info(f"Facebook import completed for user {user_id}")
                return results
                
        except Exception as e:
            logger.error(f"Error processing Facebook import: {e}")
            logger.error(traceback.format_exc())
            
            # Update data source with error
            async with AsyncSessionLocal() as db:
                await self._update_data_source_status(
                    data_source_id, "failed", db, str(e)
                )
                await db.commit()
            
            return {
                "error": str(e),
                "processing_time": 0,
                "posts_processed": 0,
                "photos_processed": 0,
                "friends_processed": 0
            }
        
        finally:
            self.is_running = False
            self.current_task = None
    
    async def _process_posts(
        self, 
        parser: FacebookDataParser, 
        validator: FacebookDataValidator,
        mapper: FacebookDataMapper,
        db: AsyncSession,
        export_path: str
    ) -> Dict[str, Any]:
        """Process Facebook posts"""
        
        results = {
            "posts_processed": 0,
            "errors": [],
            "warnings": []
        }
        
        try:
            post_count = 0
            error_count = 0
            
            async for post in parser.parse_posts():
                try:
                    # Update progress
                    post_count += 1
                    if post_count % 100 == 0:
                        await self._update_progress(
                            "posts", post_count, f"Processing post {post_count}"
                        )
                    
                    # Validate post
                    validation_result = validator.validate_post(post)
                    
                    if not validation_result.is_valid:
                        error_count += 1
                        results["errors"].extend(validation_result.errors)
                        logger.warning(f"Invalid post skipped: {validation_result.errors}")
                        continue
                    
                    if validation_result.warnings:
                        results["warnings"].extend(validation_result.warnings)
                    
                    # Map to database
                    memory = await mapper.map_post_to_memory(
                        validation_result.sanitized_data, db, export_path
                    )
                    
                    if memory:
                        results["posts_processed"] += 1
                    else:
                        error_count += 1
                        results["errors"].append(f"Failed to map post {post_count}")
                    
                    # Commit periodically
                    if post_count % 50 == 0:
                        await db.commit()
                    
                except Exception as e:
                    error_count += 1
                    error_msg = f"Error processing post {post_count}: {str(e)}"
                    results["errors"].append(error_msg)
                    logger.error(error_msg)
                    continue
            
            # Final commit
            await db.commit()
            
            logger.info(f"Posts processing completed: {results['posts_processed']} processed, {error_count} errors")
            
        except Exception as e:
            logger.error(f"Error in posts processing: {e}")
            results["errors"].append(f"Posts processing failed: {str(e)}")
        
        return results
    
    async def _process_photos(
        self, 
        parser: FacebookDataParser, 
        validator: FacebookDataValidator,
        mapper: FacebookDataMapper,
        db: AsyncSession,
        export_path: str
    ) -> Dict[str, Any]:
        """Process Facebook photos"""
        
        results = {
            "photos_processed": 0,
            "errors": [],
            "warnings": []
        }
        
        try:
            photo_count = 0
            error_count = 0
            
            async for photo in parser.parse_photos():
                try:
                    # Update progress
                    photo_count += 1
                    if photo_count % 50 == 0:
                        await self._update_progress(
                            "photos", photo_count, f"Processing photo {photo_count}"
                        )
                    
                    # Validate photo
                    validation_result = validator.validate_photo(photo, export_path)
                    
                    if not validation_result.is_valid:
                        error_count += 1
                        results["errors"].extend(validation_result.errors)
                        logger.warning(f"Invalid photo skipped: {validation_result.errors}")
                        continue
                    
                    if validation_result.warnings:
                        results["warnings"].extend(validation_result.warnings)
                    
                    # Map to database
                    memory = await mapper.map_photo_to_memory(
                        validation_result.sanitized_data, db, export_path
                    )
                    
                    if memory:
                        results["photos_processed"] += 1
                    else:
                        error_count += 1
                        results["errors"].append(f"Failed to map photo {photo_count}")
                    
                    # Commit periodically
                    if photo_count % 25 == 0:
                        await db.commit()
                    
                except Exception as e:
                    error_count += 1
                    error_msg = f"Error processing photo {photo_count}: {str(e)}"
                    results["errors"].append(error_msg)
                    logger.error(error_msg)
                    continue
            
            # Final commit
            await db.commit()
            
            logger.info(f"Photos processing completed: {results['photos_processed']} processed, {error_count} errors")
            
        except Exception as e:
            logger.error(f"Error in photos processing: {e}")
            results["errors"].append(f"Photos processing failed: {str(e)}")
        
        return results
    
    async def _process_friends(
        self, 
        parser: FacebookDataParser, 
        validator: FacebookDataValidator,
        mapper: FacebookDataMapper,
        db: AsyncSession
    ) -> Dict[str, Any]:
        """Process Facebook friends"""
        
        results = {
            "friends_processed": 0,
            "errors": [],
            "warnings": []
        }
        
        try:
            friend_count = 0
            error_count = 0
            
            async for friend in parser.parse_friends():
                try:
                    # Update progress
                    friend_count += 1
                    if friend_count % 100 == 0:
                        await self._update_progress(
                            "friends", friend_count, f"Processing friend {friend_count}"
                        )
                    
                    # Validate friend
                    validation_result = validator.validate_friend(friend)
                    
                    if not validation_result.is_valid:
                        error_count += 1
                        results["errors"].extend(validation_result.errors)
                        logger.warning(f"Invalid friend skipped: {validation_result.errors}")
                        continue
                    
                    if validation_result.warnings:
                        results["warnings"].extend(validation_result.warnings)
                    
                    # Map to database
                    person = await mapper.map_friend_to_person(
                        validation_result.sanitized_data, db
                    )
                    
                    if person:
                        results["friends_processed"] += 1
                    else:
                        error_count += 1
                        results["errors"].append(f"Failed to map friend {friend_count}")
                    
                    # Commit periodically
                    if friend_count % 100 == 0:
                        await db.commit()
                    
                except Exception as e:
                    error_count += 1
                    error_msg = f"Error processing friend {friend_count}: {str(e)}"
                    results["errors"].append(error_msg)
                    logger.error(error_msg)
                    continue
            
            # Final commit
            await db.commit()
            
            logger.info(f"Friends processing completed: {results['friends_processed']} processed, {error_count} errors")
            
        except Exception as e:
            logger.error(f"Error in friends processing: {e}")
            results["errors"].append(f"Friends processing failed: {str(e)}")
        
        return results
    
    async def _update_data_source_status(
        self, 
        data_source_id: str, 
        status: str, 
        db: AsyncSession,
        error_log: Optional[str] = None
    ):
        """Update data source status"""
        
        update_data = {
            "status": status,
            "updated_at": datetime.utcnow()
        }
        
        if status == "processing":
            update_data["processing_started_at"] = datetime.utcnow()
        elif status in ["completed", "failed"]:
            update_data["processing_completed_at"] = datetime.utcnow()
        
        if error_log:
            update_data["error_log"] = error_log
        
        await db.execute(
            update(DataSource)
            .where(DataSource.id == data_source_id)
            .values(**update_data)
        )
    
    async def _store_analysis_results(
        self, 
        data_source_id: str, 
        analysis: Dict[str, Any], 
        db: AsyncSession
    ):
        """Store export analysis results"""
        
        # Update data source with analysis info
        await db.execute(
            update(DataSource)
            .where(DataSource.id == data_source_id)
            .values(
                total_items=analysis.get("file_counts", {}).get("posts", 0) + 
                           analysis.get("media_counts", {}).get("total_images", 0) +
                           analysis.get("file_counts", {}).get("friends_and_followers", 0),
                file_size=analysis.get("total_size_bytes", 0)
            )
        )
        
        # Store detailed analysis in Redis for progress tracking
        await redis_service.set_cache(
            f"facebook_analysis:{data_source_id}",
            analysis,
            expire=24 * 60 * 60  # 24 hours
        )
    
    async def _update_final_statistics(
        self, 
        data_source_id: str, 
        results: Dict[str, Any],
        mapping_stats: Dict[str, Any],
        db: AsyncSession
    ):
        """Update final processing statistics"""
        
        total_processed = (
            results.get("posts_processed", 0) +
            results.get("photos_processed", 0) +
            results.get("friends_processed", 0)
        )
        
        total_failed = len(results.get("errors", []))
        
        await db.execute(
            update(DataSource)
            .where(DataSource.id == data_source_id)
            .values(
                processed_items=total_processed,
                failed_items=total_failed
            )
        )
        
        # Store detailed results in Redis
        final_results = {
            **results,
            "mapping_stats": mapping_stats,
            "completed_at": datetime.utcnow().isoformat()
        }
        
        await redis_service.set_cache(
            f"facebook_results:{data_source_id}",
            final_results,
            expire=7 * 24 * 60 * 60  # 7 days
        )
    
    async def _update_progress(self, data_type: str, count: int, message: str):
        """Update processing progress"""
        
        if not self.current_task:
            return
        
        progress_data = {
            "data_type": data_type,
            "count": count,
            "message": message,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Store in Redis for real-time progress tracking
        await redis_service.set_cache(
            f"facebook_progress:{self.current_task['data_source_id']}",
            progress_data,
            expire=60 * 60  # 1 hour
        )
        
        logger.debug(f"Progress update: {message}")
    
    async def get_import_status(self, data_source_id: str) -> Dict[str, Any]:
        """Get current import status"""
        
        # Get progress from Redis
        progress = await redis_service.get_cache(f"facebook_progress:{data_source_id}")
        
        # Get analysis from Redis
        analysis = await redis_service.get_cache(f"facebook_analysis:{data_source_id}")
        
        # Get results if completed
        results = await redis_service.get_cache(f"facebook_results:{data_source_id}")
        
        # Get data source status from database
        async with AsyncSessionLocal() as db:
            data_source = await db.execute(
                select(DataSource).where(DataSource.id == data_source_id)
            )
            data_source = data_source.scalar_one_or_none()
        
        status_data = {
            "data_source_id": data_source_id,
            "status": data_source.status if data_source else "unknown",
            "progress": progress,
            "analysis": analysis,
            "results": results,
            "is_running": self.is_running and self.current_task and 
                         self.current_task["data_source_id"] == data_source_id
        }
        
        if data_source:
            status_data.update({
                "total_items": data_source.total_items,
                "processed_items": data_source.processed_items,
                "failed_items": data_source.failed_items,
                "processing_started_at": data_source.processing_started_at.isoformat() 
                                       if data_source.processing_started_at else None,
                "processing_completed_at": data_source.processing_completed_at.isoformat() 
                                         if data_source.processing_completed_at else None,
                "error_log": data_source.error_log
            })
        
        return status_data
    
    def stop_import(self):
        """Stop current import process"""
        if self.is_running:
            logger.info("Stopping Facebook import process...")
            self.is_running = False
            # Note: Actual stopping would require more sophisticated cancellation
            # This is a simplified version

# Global worker instance
facebook_import_worker = FacebookImportWorker()
```

**Deliverables:**
- [ ] Asynchronous import worker with progress tracking
- [ ] Batch processing with periodic commits
- [ ] Error handling and recovery mechanisms
- [ ] Real-time progress updates via Redis
- [ ] Import status monitoring and reporting
- [ ] Graceful shutdown and cancellation support

---

## Final Deliverables Summary

### 1. Facebook Export Analysis
- [ ] Export structure analyzer with version detection
- [ ] File format validation and compatibility checking
- [ ] Size estimation and processing time calculation
- [ ] Potential issue detection and warnings

### 2. Data Parsing & Validation
- [ ] Comprehensive JSON parser for all Facebook data types
- [ ] Text encoding/decoding utilities for international content
- [ ] Robust data validation with privacy filtering
- [ ] Content sanitization and sensitive information detection
- [ ] Media file validation and verification

### 3. Database Integration
- [ ] Facebook-to-database entity mapping service
- [ ] People and places management with deduplication
- [ ] Media file processing and storage integration
- [ ] AI content generation (summaries, tags, embeddings)
- [ ] Vector database integration for search

### 4. Import Processing Pipeline
- [ ] Asynchronous worker with batch processing
- [ ] Real-time progress tracking and status updates
- [ ] Error handling and partial import recovery
- [ ] Performance optimization for large datasets
- [ ] Import statistics and reporting

### 5. API Integration
- [ ] Import initiation and management endpoints
- [ ] Progress monitoring and status checking
- [ ] Import history and statistics
- [ ] Error reporting and troubleshooting

### 6. Privacy & Security
- [ ] Privacy-first data processing
- [ ] Sensitive content filtering and redaction
- [ ] User consent and data control
- [ ] Secure file handling and cleanup

---

## Success Validation

To validate successful completion of this epic:

1. **Data Completeness**: Successfully import 95%+ of valid Facebook data
2. **Performance**: Process 1GB of data in < 30 minutes
3. **Error Handling**: Graceful handling of corrupted or incomplete data
4. **Privacy Compliance**: Proper filtering and handling of sensitive content
5. **Progress Tracking**: Real-time status updates and accurate progress reporting
6. **Data Quality**: Imported data properly mapped and searchable
7. **Recovery**: Ability to resume interrupted imports

---

**Epic Owner**: Backend Developer + Data Processing Engineer  
**Stakeholders**: Product Team, Privacy Team, QA Team  
**Dependencies**: 1.1.2 Database Architecture, 1.1.3 Core API Framework  
**Risk Level**: Medium-High (data complexity, privacy requirements, performance)