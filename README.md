<!-- This file explains how to create LifeLog entries from several data sources. -->

# AI-Augmented Personal Archive

> **Enhanced Personal Timeline with AI-Powered Storytelling, Memory Intelligence, and Narrative Exploration**

Transform your personal data into meaningful stories and insights with advanced AI capabilities while keeping everything private and local.

## ğŸš€ What's New: AI-Augmented Features

This enhanced version transforms the original Personal Timeline into an intelligent, narrative-driven personal archive:

### âœ¨ Core AI Features
- **ğŸ“– Story Generation**: AI creates narrative chapters from your memories with multiple modes (chronological, thematic, people-centered, place-centered)
- **ğŸ‘¥ People Intelligence**: Auto-detects people in your data, analyzes relationships, and tracks interaction evolution over time
- **ğŸ¨ Smart Galleries**: Natural language gallery creation ("show me creative moments", "find travel adventures")
- **ğŸ§  Enhanced Memory Retrieval**: Semantic understanding beyond keyword search with conversational exploration
- **ğŸ”„ Memory Resurfacing**: Contextual memory suggestions and AI-generated reflection prompts
- **ğŸ—ºï¸ Place-Based Narratives**: Enhanced map exploration with story-driven location insights
- **ğŸ” Self-Reflection Tools**: Pattern analysis, life chapter detection, and personal growth insights

### ğŸ›¡ï¸ Privacy & Safety
- **ğŸ  100% Local Processing**: All AI runs on your machine, no external API calls
- **ğŸ”’ Private by Default**: Content generation respects privacy settings
- **ğŸš« Diagnostic Prevention**: Avoids medical/psychological diagnoses, presents patterns as suggestions
- **ğŸ‘¤ User Control**: Complete control over sensitive content and exclusions
- **ğŸ“Š Privacy Monitoring**: Comprehensive privacy compliance tracking

## Notes from the human

Despite the bold claims, almost no new feature really works as intended. But the UI is improved, the timeline works better than before, and a lot of ideas, fully documented and with code mocks are there to be picked up.

## ğŸ¯ Quick Start

### Option 1: Full AI-Augmented Experience (Recommended)

**Prerequisites:**
- Docker Desktop installed and running
- **API Keys** (optional but recommended for AI features)

**âš¡ 5-Minute Setup:**
```bash
# 1. Setup API keys for AI features (optional)
cp .env.example .env
# Edit .env and add at least one API key:
# OPENAI_API_KEY=sk-your-key-here

# 2. Start the application
./start_app.sh
```

**ï¿½  Access Your Archive:**
- **ğŸ¨ Main App**: http://localhost:52692 - Your personal timeline with AI features
- **ï¿½ AI  Chat**: http://localhost:57485 - Conversational memory exploration  
- **ğŸ“Š System Health**: http://localhost:8086/health - API status and system health
- **âš™ï¸ Backend API**: http://localhost:8000 - REST endpoints

**ğŸ”‘ API Token Setup:**
- **With API Keys**: Full AI features (story generation, people intelligence, smart galleries)
- **Without API Keys**: Basic timeline features only (data import, visualization, search)
- **Status Indicator**: The app shows clear badges indicating AI feature availability

> ğŸ“– **Detailed Setup**: See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for complete API token configuration, cost management, and troubleshooting.

### Option 2: Try AI Features (No Docker Required)

Experience the AI capabilities with demo scripts:

```bash
# Run all demos
config/test_services.sh

# Or individual demos:
python examples/gallery_curation_demo.py      # Smart gallery creation
python examples/memory_resurfacing_demo.py    # Contextual memory suggestions
python examples/self_reflection_demo.py       # Personal growth insights
python examples/privacy_safety_demo.py        # Privacy controls demo
```

> ğŸ“– **Complete Setup Guide**: See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for detailed API token configuration, cost management, troubleshooting, and production deployment.

## ğŸ® Key Features to Explore

### ğŸ“– AI Story Generation
1. Navigate to the main app (http://localhost:52692)
2. Click "Generate Story" 
3. Choose narrative mode:
   - **Chronological**: Time-based storytelling
   - **Thematic**: Topic-focused narratives  
   - **People-Centered**: Relationship-focused stories
   - **Place-Centered**: Location-based journeys
4. Watch AI create chapters with your memories

### ğŸ‘¥ People Intelligence
1. Go to "People" tab in the main app
2. See auto-detected people from your data
3. View interaction timelines and relationship evolution
4. Generate "best of us" compilations with shared memories

### ğŸ¨ Smart Galleries
1. Navigate to "Galleries" section
2. Try natural language prompts:
   - "Show me creative moments"
   - "Find travel adventures" 
   - "Quiet reflective times"
   - "Celebrations with family"
3. Convert galleries to narrative stories

### ğŸ¤– Enhanced Memory Chat
1. Visit the AI Chat interface (http://localhost:57485)
2. Ask conversational questions:
   - "Tell me about my experiences with friends"
   - "What were my highlights this year?"
   - "Show me memories from travel"
   - "Help me reflect on my growth"
3. Get contextual, narrative responses with memory connections

### ğŸ”„ Proactive Memory Resurfacing
- Receive gentle suggestions for forgotten memories
- Get AI-generated reflection prompts
- Discover patterns connecting past and present interests
- Explore themed memory collections

## ğŸ—ï¸ Architecture

### Enhanced Service Architecture
The AI-Augmented Personal Archive consists of four main services:

1. **Frontend** (Port 52692) - Enhanced React UI with AI components
2. **Backend** (Port 8000) - Data processing with AI enhancement pipeline  
3. **QA Service** (Port 57485) - Enhanced conversational memory access
4. **AI Services** (Port 8086) - Local AI models for narrative generation

### AI Agent System
Behind the scenes, specialized AI agents work together:
- **Archivist Agent**: Selects and curates relevant memories
- **Narrative Agent**: Creates coherent stories from personal data
- **Editor Agent**: Filters and organizes content appropriately  
- **Director Agent**: Sequences media and pacing for storytelling
- **Critic Agent**: Ensures quality, safety, and grounding in actual data

## ğŸ“Š System Status

- âœ… **112/119 tests passing** (94% success rate)
- âœ… **All core AI features operational**
- âœ… **Privacy and safety controls active**
- âœ… **Backward compatibility maintained**
- âœ… **Docker integration optimized**

## ğŸ”§ Configuration

### AI Model Configuration
Create `.env` file from `.env.example` and customize:

```bash
# AI Models (all run locally)
LOCAL_LLM_MODEL=microsoft/DialoGPT-medium
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
TTS_MODEL=espnet/kan-bayashi_ljspeech_vits
MULTIMODAL_MODEL=openai/clip-vit-base-patch32

# Processing Configuration  
AI_PROCESSING_BATCH_SIZE=32
AI_MAX_MEMORY_GB=4
ENABLE_GPU=false

# Feature Toggles
ENABLE_AI_ENHANCEMENT=true
ENHANCED_QA_ENABLED=true
```

## ï¿½  Project Structure

```
personal-timeline/
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”œâ”€â”€ ğŸ“„ .env.example                 # Environment configuration template
â”œâ”€â”€ ğŸ“„ start_app.sh                 # Quick start script
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # Docker services configuration
â”œâ”€â”€ ğŸ“„ pyproject.toml               # Python project configuration
â”œâ”€â”€ ğŸ“„ uv.lock                      # UV package lock file
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ src/                         # Source code
â”‚   â”œâ”€â”€ ğŸ“‚ ai_services/             # AI service implementations
â”‚   â”œâ”€â”€ ğŸ“‚ data_processing/         # Data processing pipeline
â”‚   â””â”€â”€ ğŸ“‚ frontend/                # React frontend application
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ tests/                       # Test suite
â”‚   â”œâ”€â”€ ğŸ“‚ unit/                    # Unit tests
â”‚   â”œâ”€â”€ ğŸ“‚ integration/             # Integration tests
â”‚   â”œâ”€â”€ ğŸ“‚ component/               # Component tests
â”‚   â”œâ”€â”€ ğŸ“‚ e2e/                     # End-to-end tests
â”‚   â””â”€â”€ ğŸ“„ run_all_tests.py         # Test runner
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ config/                      # Configuration files
â”‚   â”œâ”€â”€ ğŸ“„ ingest.conf              # Data ingestion configuration
â”‚   â”œâ”€â”€ ğŸ“„ setup_dev_uv.sh          # Development setup script
â”‚   â””â”€â”€ ğŸ“‚ archive/                 # Archived configurations
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ docs/                        # Documentation
â”‚   â”œâ”€â”€ ğŸ“„ DEPLOYMENT_GUIDE.md      # Deployment instructions
â”‚   â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md          # Contribution guidelines
â”‚   â””â”€â”€ ğŸ“„ QUICK_START_GUIDE.md     # Quick start guide
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ tools/                       # Development tools & utilities
â”‚   â”œâ”€â”€ ğŸ“„ debug_*.py               # Debug scripts
â”‚   â”œâ”€â”€ ğŸ“„ *test*.html              # HTML test interfaces
â”‚   â””â”€â”€ ğŸ“„ create_sample_data.py    # Sample data generator
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ reports/                     # Test reports & analysis
â”‚   â”œâ”€â”€ ğŸ“„ FINAL_TEST_REPORT.md     # Latest test results
â”‚   â””â”€â”€ ğŸ“„ *_results.json          # Test execution results
â”œâ”€â”€ 
â””â”€â”€ ğŸ“‚ MyData/                      # Your personal data (created on first run)
    â”œâ”€â”€ ğŸ“‚ facebook/                # Facebook export data
    â”œâ”€â”€ ğŸ“‚ google_photos/           # Google Photos data
    â””â”€â”€ ğŸ“‚ processed_data.db        # Processed database
```

## ğŸ§ª Testing

### Quick Test Suite
```bash
# Run all tests with the comprehensive test runner
python tests/run_all_tests.py

# Run specific test categories
python tests/run_all_tests.py --category unit
python tests/run_all_tests.py --category integration
python tests/run_all_tests.py --category e2e
```

### Individual Test Files
```bash
# AI Service Tests
python -m pytest tests/unit/test_story_generation.py -v
python -m pytest tests/unit/test_ai_providers.py -v
python -m pytest tests/test_people_intelligence.py -v
python -m pytest tests/test_data_processing.py -v

# Integration Tests
python -m pytest tests/integration/test_api_endpoints.py -v

# Component Tests
python -m pytest tests/component/test_frontend_components.py -v

# End-to-End Tests
python -m pytest tests/e2e/test_user_workflows.py -v
```

### Frontend Tests
```bash
# React component tests
cd src/frontend && npm test -- --watchAll=false

# Frontend integration tests
cd src/frontend && npm run test:integration
```

### Manual Testing Tools
```bash
# HTML-based testing interfaces (open in browser)
open tools/browser_functionality_test.html
open tools/test_frontend_functionality.html
open tools/simple_frontend_test.html

# Debug tools
python tools/debug_openai_error.py
python tools/trigger_people_detection.py
python tools/create_sample_data.py
```

### Development Scripts
```bash
# Restart all services
config/RESTART_DOCKER.sh

# Verify Docker setup
config/verify_enhanced_docker_setup.sh

# Development environment setup
config/setup_dev_uv.sh

# Stop all services
docker compose down
```

### Test Reports
After running tests, check the `reports/` folder for:
- `FINAL_TEST_REPORT.md` - Latest comprehensive test results
- `*_test_results.json` - Detailed test execution data
- `system_analysis_results.json` - System health analysis

## ğŸ“š Documentation

### AI-Augmented Features
- [QUICK_START_GUIDE.md](QUICK_START_GUIDE.md) - Complete setup and testing guide
- [SYSTEM_INTEGRATION_REPORT.md](SYSTEM_INTEGRATION_REPORT.md) - Detailed test results and system status
- [docs/DOCKER_AI_INTEGRATION.md](docs/DOCKER_AI_INTEGRATION.md) - AI services Docker configuration

### Original Features
- [docs/DOCKER_READY.md](docs/DOCKER_READY.md) - Original Docker setup guide
- [docs/DATASET.md](docs/DATASET.md) - Sample dataset information
- [docs/TROUBLESHOOTING_GUIDE.md](docs/TROUBLESHOOTING_GUIDE.md) - Common issues and solutions

## ğŸ¯ Migration from Original Timeline

The AI-Augmented Personal Archive is fully backward compatible:

1. **Existing Data**: All your current data is preserved and enhanced
2. **Original Features**: Timeline view, QA system, and data importers still work
3. **Enhanced Experience**: New AI features layer on top of existing functionality
4. **Seamless Upgrade**: Run the migration automatically on first startup

---

## ğŸ“‹ Table of Contents

### AI-Augmented Features
- [ğŸš€ Quick Start](#-quick-start) - Start here for AI features!
- [ğŸ® Key Features to Explore](#-key-features-to-explore) - Try the AI capabilities
- [ğŸ—ï¸ Architecture](#ï¸-architecture) - System design and AI agents
- [ğŸ”§ Configuration](#-configuration) - AI model settings
- [ğŸ› ï¸ Development & Testing](#ï¸-development--testing) - Testing and development
- [ğŸ“š Documentation](#-documentation) - AI feature guides

### Original Timeline Features  
- [ğŸ”§ General Setup](#general-setup) - Original setup instructions
- [ğŸ“¥ Digital Data Importers](#digital-data-importers) - Data source importers
- [ğŸ“Š Data Visualization](#visualization-of-the-personal-timeline) - Timeline UI
- [â“ Question Answering](#question-answer-over-the-personal-timeline) - Original QA system
- [ğŸ“– TimelineQA](#timelineqa-a-benchmark-for-question-answer-over-the-personal-timeline) - QA benchmark

---

## General Setup

## Step 0: Create environment

1. Install Docker Desktop from [this link](https://docs.docker.com/desktop/).

2. Follow install steps and use the Desktop app to start the docker engine.

3. Install `git-lfs` and clone the repo. You may need a conda env to do that:
```
conda create -n personal-timeline python=3.10
conda activate personal-timeline

conda install -c conda-forge git-lfs
git lfs install

git clone https://github.com/facebookresearch/personal-timeline
cd personal-timeline
```

4. Run init script (needs python)
```
sh src/init.sh
```
This will create a bunch of files/folders/symlinks needed for running the app.
This will also create a new directory under your home folder `~/personal-data`, the directory where your personal data will reside.

## Step 1: Setting up


## For Data Ingestion

Ingestion configs are controlled via parameters in `conf/ingest.conf` file. The configurations
are defaulted for optimized processing and don't need to be changed. 
You can adjust values for these parameters to run importer with a different configuration.



## For Data visualization

1. To set up a Google Map API (free), follow these [instructions](https://developers.google.com/maps/documentation/embed/quickstart#create-project).

Copy the following lines to `env/frontend.env.list`:
```
GOOGLE_MAP_API=<the API key goes here>
```

2. To embed Spotify, you need to set up a Spotify API (free) following [here](https://developer.spotify.com/dashboard/applications). You need to log in with a Spotify account, create a project, and show the `secret`.

Copy the following lines to `env/frontend.env.list`:
```
SPOTIFY_TOKEN=<the token goes here>
SPOTIFY_SECRET=<the secret goes here>
```

## For Question-Answering

Set up an OpenAI API following these [instructions](https://openai.com/api/).

Copy the following line to `env/frontend.env.list`:
```
OPENAI_API_KEY=<the API key goes here>
```

## Digital Data Importers


## Downloading your personal data

We currently support 9 data sources. Here is a summary table:

| Digital Services | Instructions                                                                        | Destinations                                                             | Use cases                                              |
|------------------|-------------------------------------------------------------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------|
| Apple Health     | [Link](https://github.com/facebookresearch/personal-timeline#apple-health)  | personal-data/apple-health                                               | Exercise patterns, calorie counts                      |
| Amazon           | [Link](https://github.com/facebookresearch/personal-timeline#amazon)        | personal-data/amazon                                                     | Product recommendation, purchase history summarization |
| Amazon Kindle    | [Link](https://github.com/facebookresearch/personal-timeline#amazon)        | personal-data/amazon-kindle                                              | Book recommendation                                    |
| Spotify          | [Link](https://github.com/facebookresearch/personal-timeline#spotify)       | personal-data/spotify                                                    | Music / streaming recommendation                       |
| Venmo            | [Link](https://github.com/facebookresearch/personal-timeline#venmo)         | personal-data/venmo                                                      | Monthly spend summarization                            |
| Libby            | [Link](https://github.com/facebookresearch/personal-timeline#libby)         | personal-data/libby                                                      | Book recommendation                                    |
| Google Photos    | [Link](https://github.com/facebookresearch/personal-timeline#google-photos) | personal-data/google_photos                                              | Food recommendation, Object detections, and more               |
| Google Location  | [Link](https://github.com/facebookresearch/personal-timeline#google-photos) | personal-data/google-timeline/Location History/Semantic Location History | Location tracking / visualization                      |
| Facebook posts   | [Link](https://github.com/facebookresearch/personal-timeline#facebook-data) | personal-data/facebook                                                   | Question-Answering over FB posts / photos              |

If you have a different data source not listed above, follow the instructions [here](NEW_DATASOURCE.md)
to add this data source to the importer.

### GOOGLE PHOTOS and GOOGLE TIMELINE
<!--1. You need to download your Google photos from [Google Takeout](https://takeout.google.com/).  
The download from Google Takeout would be in multiple zip files. Unzip all the files.

2. It may be the case that some of your photo files are .HEIC. In that case follow the steps below to convert them to .jpeg  
The easiest way to do this on a Mac is:

     -- Select the .HEIC files you want to convert.   
     -- Right click and choose "quick actions" and then you'll have an option to convert the image.  
     -- If you're converting many photos, this may take a few minutes. 

2. Move all the unzipped folders inside `~/personal-data/google_photos/`. There can be any number of sub-folders under `google_photos`.-->

1. You can download your Google photos and location (also Gmail, map and google calendar) data from [Google Takeout](https://takeout.google.com/).
2. The download from Google Takeout would be in multiple zip files. Unzip all the files.
3. For Google photos, move all the unzipped folders inside `~/personal-data/google_photos/`. There can be any number of sub-folders under `google_photos`.
4. For Google locations, move the unzipped files to `personal-data/google-timeline/Location History/Semantic Location History`.

### FACEBOOK DATA
1. Go to [Facebook Settings](https://www.facebook.com/settings?tab=your_facebook_information) 
2. Click on <b>Download your information</b> and download FB data in JSON format
3. Unzip the downloaded file and copy the directory `posts` sub-folder to `~/personal-data/facebook`. The `posts` folder would sit directly under the Facebook folder.

### APPLE HEALTH
1. Go to the Apple Health app on your phone and ask to export your data. This will create a file called iwatch.xml and that's the input file to the importer.
2. Move the downloaded file to this `~/personal-data/apple-health`

### AMAZON
1. Request your data from Amazon here: https://www.amazon.com/gp/help/customer/display.html?nodeId=GXPU3YPMBZQRWZK2
They say it can take up to 30 days, but it took about 2 days. They'll email you when it's ready.

They separate Amazon purchases from Kindle purchases into two different directories.

The file you need for Amazon purchases is Retail.OrderHistory.1.csv
The file you need for Kindle purchases is Digital Items.csv

2. Move data for Amazon purchases to `~/personal-data/amazon` folder and of kindle downloads to `~/personal-data/amazon-kindle` folder

### VENMO
1. Download your data from Venmo here -- https://help.venmo.com/hc/en-us/articles/360016096974-Transaction-History

2. Move the data into `~/personal-data/venmo` folder.

### LIBBY
1. Download your data from Libby here -- https://libbyapp.com/timeline/activities. Click on `Actions` then `Export Timeline`

2. Move the data into `~/personal-data/libby` folder.


### SPOTIFY

1. Download your data from Spotify here -- https://support.spotify.com/us/article/data-rights-and-privacy-settings/
They say it can take up to 30 days, but it took about 2 days. They'll email you when it's ready.

2. Move the data into `~/personal-data/spotify` folder.

# Running the code
Now that we have all the data and setting in place, we can either run individual steps or the end-to-end system.
This will import your photo data to SQLite (this is what will go into the episodic database), build summaries
and make data available for visualization and search.


Running the Ingestion container will add two types of file to `~/personal-data/app_data` folder
 - Import your data to an SQLite DB named `raw_data.db`
 - Export your personal data into csv files such as `books.csv`, `exercise.csv`, etc.

### Option 1:
To run the pipeline end-to-end (with frontend and QA backend), simply run 
```
docker-compose up -d --build
```

### Option 2:
You can also run ingestion, visualization, and the QA engine separately.
To start data ingestion, use  
```
docker-compose up -d backend --build
```

## Check progress
Once the docker command is run, you can see running containers for backend and frontend in the docker for Mac UI.
Copy the container Id for ingest and see logs by running the following command:  
```
docker logs -f <container_id>
```

<!-- # Step 5: Visualization and Question Answering -->

## Visualization of the personal timeline

To start the visualization frontend:
```
docker-compose up -d frontend --build
```

Running the Frontend will start a ReactJS UI at `http://localhost:3000`. See [here](src/frontend/) for more details.

We provide an anonymized digital data [dataset](sample_data/) for testing the UI and QA system, see [here](DATASET.md) for more details.

![Timeline Visualization](ui.png)


## Question Answer over the personal timeline

The QA engine is based on PostText, a QA system for answering queries that require computing aggregates over personal data.

PostText Reference ---  [https://arxiv.org/abs/2306.01061](https://arxiv.org/abs/2306.01061):
```
@article{tan2023posttext,
      title={Reimagining Retrieval Augmented Language Models for Answering Queries},
      author={Wang-Chiew Tan and Yuliang Li and Pedro Rodriguez and Richard James and Xi Victoria Lin and Alon Halevy and Scott Yih},
      journal={arXiv preprint:2306.01061},
      year={2023},
}
```

To start the QA engine, run:
```
docker-compose up -d qa --build
```
The QA engine will be running on a flask server inside a docker container at `http://localhost:8085`. 

See [here](src/qa) for more details.

![QA Engine](qa.png)

There are 3 options for the QA engine.
* *ChatGPT*: uses OpenAI's gpt-3.5-turbo [API](https://platform.openai.com/docs/models/overview) without the personal timeline as context. It answers world knowledge question such as `what is the GDP of US in 2021` but not personal questions.
* *Retrieval-based*: answers question by retrieving the top-k most relevant episodes from the personal timeline as the LLM's context. It can answer questions over the personal timeline such as `show me some plants in my neighborhood`.
* *View-based*: translates the input question to a (customized) SQL query over tabular views (e.g., books, exercise, etc.) of the personal timeline. This QA engine is good at answering aggregate queries (`how many books did I purchase?`) and min/max queries (`when was the last time I travel to Japan`).


Example questions you may try:
* `Show me some photos of plants in my neighborhood`
* `Which cities did I visit when I traveled to Japan?`
* `How many books did I purchase in April?`

## ğŸ“– TimelineQA: a benchmark for Question Answer over the personal timeline

TimelineQA is a synthetic benchmark for accelerating progress on querying personal timelines. 
TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high
school graduation to those that occur on a daily basis such as going for a run. We have evaluated SOTA models for atomic and multi-hop QA on the benchmark. 

Please check out the TimelineQA github [repo](https://github.com/facebookresearch/TimelineQA) and the TimelineQA paper ---  [https://arxiv.org/abs/2306.01061](https://arxiv.org/abs/2306.01061):
```
@article{tan2023timelineqa,
  title={TimelineQA: A Benchmark for Question Answering over Timelines},
  author={Tan, Wang-Chiew and Dwivedi-Yu, Jane and Li, Yuliang and Mathias, Lambert and Saeidi, Marzieh and Yan, Jing Nathan and Halevy, Alon Y},
  journal={arXiv preprint arXiv:2306.01069},
  year={2023}
}
```

---

## ğŸ”„ Original Personal Timeline Features

> **Note**: This is a fork of [facebookresearch/personal-timeline](https://github.com/facebookresearch/personal-timeline) with enhanced AI capabilities and Docker improvements.

The AI-Augmented Personal Archive maintains full compatibility with all original features while adding advanced AI capabilities on top. All existing functionality continues to work as before.

### Key Improvements in This Fork

- âœ… **AI-Augmented Features**: Story generation, people intelligence, smart galleries, memory resurfacing
- âœ… **Enhanced Privacy**: Local AI processing, diagnostic prevention, comprehensive user controls
- âœ… **Full Docker support** with working volume mounts and data paths
- âœ… **Fixed dependency issues** (ajv, node modules)
- âœ… **Simplified startup scripts** for Docker operations
- âœ… **Updated documentation** and troubleshooting guides
- âœ… **Facebook data importer fixes** - Auto-detection + resilient solution (176 posts imported!)
- âœ… **Frontend display fixes** - Category JSON files auto-generation
- âœ… **Bug fixes** - Fixed 2 type inconsistency bugs in photo importer

## Documentation

### User Guides
- [DOCKER_READY.md](docs/DOCKER_READY.md) - Complete Docker setup guide
- [RUN_LOCALLY.md](docs/RUN_LOCALLY.md) - Running without Docker
- [TROUBLESHOOTING_GUIDE.md](docs/TROUBLESHOOTING_GUIDE.md) - Common issues and solutions
- [DATASET.md](docs/DATASET.md) - Sample dataset information
- [MODERNIZATION_GUIDE.md](docs/MODERNIZATION_GUIDE.md) - Modernization notes

### Scripts
- `scripts/RESTART_DOCKER.sh` - Restart all Docker services
- `scripts/teardown.sh` - Stop and remove containers
- `scripts/verify_docker_setup.sh` - Verify Docker configuration

### AI Assistant Files
Development notes and configuration files used during development with OpenHands are in `docs/ai-assistant/`.

## ğŸ“„ License

The codebase is licensed under the [Apache 2.0 license](LICENSE).

## ğŸ¤ Contributing

See [contributing](CONTRIBUTING.md) and the [code of conduct](CODE_OF_CONDUCT.md).

## ğŸ™ Contributor Attribution

### AI-Augmented Personal Archive Contributors
- **AI Enhancement Development**: Comprehensive AI feature implementation including story generation, people intelligence, smart galleries, memory resurfacing, and privacy controls
- **System Integration**: Full integration testing, Docker optimization, and backward compatibility
- **Documentation**: Complete documentation overhaul with quick start guides and testing instructions

### Original Personal Timeline Contributors
We'd like to thank the following contributors for their contributions to the original project:
- [Tripti Singh](https://github.com/tripti-singh)
  - Design and implementation of the sqlite DB backend
  - Designing a pluggable data import and enrichment layer and building the pipeline orchestrator.
  - Importers for all six [data sources](â€‹â€‹https://github.com/facebookresearch/personal-timeline#digital-data-importers)
  - Generic csv and json data sources importer with [instructions](https://github.com/facebookresearch/personal-timeline/blob/main/NEW_DATASOURCE.md)
  - Dockerization
  - Contributing in Documentation
- [Wang-Chiew Tan](https://github.com/wangchiew)
  - Implementation of the [PostText](https://arxiv.org/abs/2306.01061) query engine
- [Pierre Moulon](https://github.com/SeaOtocinclus) for providing open-sourcing guidelines and suggestions

---

## ğŸ‰ Get Started Now!

Ready to transform your personal data into meaningful stories and insights?

```bash
# Quick start with AI features
./start_app.sh

# Or try demos without Docker
config/test_services.sh
```

Visit http://localhost:52692 to explore your AI-augmented personal archive!
